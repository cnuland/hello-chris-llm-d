apiVersion: apps/v1
kind: Deployment
metadata:
  name: ms-llm-d-modelservice-decode
  namespace: llm-d
  labels:
    app.kubernetes.io/name: llm-d-decode
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: ms-llm-d-modelservice
    llm-d.ai/role: decode
spec:
  replicas: 3
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: ms-llm-d-modelservice
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: ms-llm-d-modelservice
        llm-d.ai/role: decode
    spec:
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d:v0.2.0
        command: ["vllm", "serve", "meta-llama/Llama-3.2-3B-Instruct"]
        args:
        - --host
        - 0.0.0.0
        - --port
        - "8000"
        - --enable-prefix-caching
        - --prefix-caching-hash-algo
        - builtin
        - --block-size
        - "16"
        - --enforce-eager
        - --no-enable-chunked-prefill
        - --kv-transfer-config
        - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: llm-d-hf-token
              key: HF_TOKEN
        - name: PYTHONHASHSEED
          value: "42"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        ports:
        - containerPort: 8000
          name: vllm
        - containerPort: 5557
          name: nixl
        # Metrics are exposed on port 8000 (/metrics)
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /v1/models
            port: 8000
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 60
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"

