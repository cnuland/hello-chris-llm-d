apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-backend
  namespace: llm-d
  labels:
    app: llm-d-backend
    component: backend
    part-of: llm-d-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-d-backend
  template:
    metadata:
      labels:
        app: llm-d-backend
        component: backend
    spec:
      containers:
      - name: backend
        image: quay.io/cnuland/llm-interface:backend-amd64
        imagePullPolicy: Always
        ports:
        - containerPort: 3001
          name: http
          protocol: TCP
        env:
        - name: NODE_ENV
          value: "production"
        - name: PORT
          value: "3001"
        - name: LLM_SERVICE_URL
          value: "http://llama-3-2-1b-decode-service.llm-d.svc.cluster.local:8000"
        - name: EPP_SERVICE_URL
          value: "http://llama-3-2-1b-epp-service.llm-d.svc.cluster.local:9002"
        - name: PROMETHEUS_URL
          value: "http://prometheus-server.monitoring.svc.cluster.local:80"
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 3001
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 3001
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
      restartPolicy: Always
