apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: llm-d
  name: llm-d-modelservice-metrics
  namespace: llm-d
spec:
  endpoints:
    # Prefill metrics endpoint (port 8000)
    - interval: 15s
      path: /metrics
      port: vllm
      scheme: http
    # Decode metrics endpoint (port 8000 via routing proxy)
    - interval: 15s
      path: /metrics
      port: vllm
      scheme: http
  selector:
    matchLabels:
      llmd.ai/gather-metrics: "true"
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: llm-d
  name: llm-d-epp-service-metrics
  namespace: llm-d
spec:
  endpoints:
    # EPP metrics endpoint
    - interval: 15s
      path: /metrics
      port: metrics
      scheme: http
  selector:
    matchLabels:
      llmd.ai/gather-metrics: "true"
      app.kubernetes.io/gateway: llm-d-operator-inference-gateway
