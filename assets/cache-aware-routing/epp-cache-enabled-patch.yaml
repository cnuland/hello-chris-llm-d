apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-3-2-1b-epp
  namespace: llm-d
spec:
  template:
    spec:
      containers:
      - name: manager
        env:
        # Enable KV-Cache Aware Routing (KEY FEATURE)
        - name: ENABLE_KVCACHE_AWARE_SCORER
          value: "true"  # ENABLED: Routes requests to pods with relevant cache
        - name: KVCACHE_AWARE_SCORER_WEIGHT
          value: "5"     # HIGHEST WEIGHT: Cache locality is most important
        
        # Prefix-aware routing (already enabled)
        - name: ENABLE_PREFIX_AWARE_SCORER
          value: "true"
        - name: PREFIX_AWARE_SCORER_WEIGHT
          value: "3"     # HIGH WEIGHT: Route based on prompt prefixes
        
        # Load-aware routing (for overflow)
        - name: ENABLE_LOAD_AWARE_SCORER
          value: "true"
        - name: LOAD_AWARE_SCORER_WEIGHT
          value: "1"     # LOW WEIGHT: Only when cache doesn't help
        
        # Session affinity (for conversation continuity)
        - name: ENABLE_SESSION_AWARE_SCORER
          value: "true"
        - name: SESSION_AWARE_SCORER_WEIGHT
          value: "2"     # MEDIUM WEIGHT: Keep conversations on same pod
