apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama-3-2-1b
  namespace: llm-d
spec:
  # Reference to the improved ConfigMap with P/D disaggregation enabled
  baseConfigMapRef:
    name: basic-gpu-with-nixl-and-pd-preset
  
  # Model configuration
  modelArtifacts:
    uri: "hf://meta-llama/Llama-3.2-1B"
    size: 50Gi
    authSecretName: "llm-d-hf-token"
  
  # Decode phase configuration - SCALED TO 3 REPLICAS
  decode:
    replicas: 3  # Increased from 1 to 3 for cache demonstration
    acceleratorTypes:
      labelKey: nvidia.com/gpu.present
      labelValues:
      - "true"
    containers:
    - name: vllm
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
  
  # Prefill phase configuration - SCALED TO 2 REPLICAS
  prefill:
    replicas: 2  # Increased from 1 to 2 for better throughput
    containers:
    - name: vllm
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
  
  # Routing configuration
  routing:
    modelName: llama-3-2-1b
    gatewayRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: llm-d-gateway
      namespace: llm-d
