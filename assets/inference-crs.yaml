apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: llm-d
  namespace: llm-d
spec:
  # Select model server pods for this demo
  selector:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: ms-llm-d-modelservice
  # vLLM HTTP port exposed by the decode/prefill pods
  targetPortNumber: 8000
  # Wire the external processing (EPP) service for cache-aware routing
  extensionRef:
    group: ""
    kind: Service
    name: ms-llm-d-modelservice-epp
    portNumber: 9002
---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  name: ms-llm-d-3b-instruct
  namespace: llm-d
spec:
  # This is the model identifier expected in incoming requests
  modelName: meta-llama/Llama-3.2-3B-Instruct
  poolRef:
    name: llm-d
  # Optional: set a default criticality; can be omitted
  criticality: Standard

