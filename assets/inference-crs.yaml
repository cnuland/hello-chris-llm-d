# NOTE: These CRs are not used in the Istio-only routing path.
# They are retained here for reference if you later enable kGateway-based InferencePool routing.
# For the current demo, routing is: HTTPRoute -> Service (decode) with Envoy ext_proc -> EPP.
#
# apiVersion: inference.networking.x-k8s.io/v1alpha2
# kind: InferencePool
# metadata:
#   name: llm-d
#   namespace: llm-d
# spec:
#   selector:
#     llm-d.ai/inferenceServing: "true"
#     llm-d.ai/model: ms-llm-d-modelservice
#   targetPortNumber: 8000
#   extensionRef:
#     group: ""
#     kind: Service
#     name: ms-llm-d-modelservice-epp
#     portNumber: 9002
# ---
# apiVersion: inference.networking.x-k8s.io/v1alpha2
# kind: InferenceModel
# metadata:
#   name: ms-llm-d-3b-instruct
#   namespace: llm-d
# spec:
#   modelName: meta-llama/Llama-3.2-3B-Instruct
#   poolRef:
#     name: llm-d
#   criticality: Standard

