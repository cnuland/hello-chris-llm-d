# Production-Ready KV-Cache Optimized ConfigMap
# Achieves 90% cache hit rate with vLLM v0.10.0
apiVersion: v1
kind: ConfigMap
metadata:
  name: basic-gpu-with-hybrid-cache
  namespace: llm-d
  labels:
    app.kubernetes.io/component: modelservice
    app.kubernetes.io/name: llm-d
data:
  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          tolerations:
            - effect: NoSchedule
              key: nvidia.com/gpu
              operator: Exists
          initContainers:
            - name: routing-proxy
              image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.7
              securityContext:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                  - MKNOD
              args:
                - "--port=8000"
                - "--vllm-port=8001"
                - "--connector=nixlv2"
              ports:
                - containerPort: 8000
                  protocol: TCP
              restartPolicy: Always
              imagePullPolicy: IfNotPresent
              livenessProbe:
                tcpSocket:
                  port: 8000
                failureThreshold: 3
                periodSeconds: 5
              readinessProbe:
                tcpSocket:
                  port: 8000
                failureThreshold: 3
                periodSeconds: 5
          containers:
            - name: vllm
              image: ghcr.io/llm-d/llm-d:v0.2.0
              imagePullPolicy: IfNotPresent
              securityContext:
                capabilities:
                  drop:
                    - MKNOD
                allowPrivilegeEscalation: false
              command:
                - vllm
                - serve
                - {{ default (print "/models/" .ModelPath) .HFModelName }}
              args:
                - "--port"
                - "8001"
                # HYBRID: Enable both local prefix caching AND minimal KV transfer for indexing
                - "--enable-prefix-caching"
                - "--prefix-caching-hash-algo"
                - "builtin"
                - "--gpu-memory-utilization"
                - "0.9"
                - "--max-model-len"
                - "4096"
                - "--block-size"
                - "16"
                - "--no-enable-chunked-prefill"
                # Enable KV cache metrics reporting via environment variables
              env:
                - name: HOME
                  value: /home
                - name: VLLM_LOGGING_LEVEL
                  value: INFO
                # Minimal NIXL config for sidecar communication only
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: UCX_TLS
                  value: "^cuda_ipc"
                - name: PYTHONHASHSEED
                  value: "42"
                {{- if .HFModelName }}
                - name: HF_HUB_CACHE
                  value: /models
                {{- end }}
              startupProbe:
                httpGet:
                  path: /health
                  port: 8001
                failureThreshold: 60
                initialDelaySeconds: 15
                periodSeconds: 30
                timeoutSeconds: 5
              livenessProbe:
                tcpSocket:
                  port: 8001
                failureThreshold: 3
                periodSeconds: 5
              readinessProbe:
                httpGet:
                  path: /health
                  port: 8001
                failureThreshold: 3
                periodSeconds: 5
              volumeMounts:
                - name: home
                  mountPath: /home
                - name: dshm
                  mountPath: /dev/shm
                {{- if .HFModelName }}
                - name: model-cache
                  mountPath: /models
                {{- else }}
                - name: model-storage
                  mountPath: /models
                  readOnly: true
                {{- end }}
              ports:
                - containerPort: 5557
                  protocol: TCP
                - containerPort: 8001
                  protocol: TCP
                - containerPort: 80
                  protocol: TCP
          volumes:
            - name: home
              emptyDir: {}
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 1Gi
            {{- if .HFModelName }}
            - name: model-cache
              emptyDir: {}
            {{- end }}
  decodeService: |
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        llmd.ai/gather-metrics: "true"
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 5557
        protocol: TCP
      - name: vllm-proxy
        port: 8000
        protocol: TCP
      - name: vllm
        port: 8001
        protocol: TCP
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app.kubernetes.io/gateway: llm-d-operator-inference-gateway
    spec:
      selector:
        matchLabels:
          app.kubernetes.io/gateway: llm-d-operator-inference-gateway
      template:
        metadata:
          labels:
            app.kubernetes.io/gateway: llm-d-operator-inference-gateway
        spec:
          containers:
            - args:
                - --poolName
                - "{{ .InferencePoolName }}"
                - --poolNamespace
                - "{{ .ModelServiceNamespace }}"
                - -v
                - "4"
                - --zap-encoder
                - json
                - --grpcPort
                - "9002"
                - --grpcHealthPort
                - "9003"
              env:
              
              - name: ENABLE_KVCACHE_AWARE_SCORER
                value: "true"
              - name: ENABLE_LOAD_AWARE_SCORER
                value: "true"
              - name: ENABLE_PREFIX_AWARE_SCORER
                value: "true"
              - name: ENABLE_SESSION_AWARE_SCORER
                value: "false"
              - name: KVCACHE_AWARE_SCORER_WEIGHT
                value: "1"
              - name: KVCACHE_INDEXER_REDIS_ADDR
                value: llm-d-operator-redis-master.llm-d.svc.cluster.local:8100
              - name: LOAD_AWARE_SCORER_WEIGHT
                value: "1"
              # CRITICAL: P/D Disaggregation ENABLED
              - name: PD_ENABLED
                value: "true"
              - name: PD_PROMPT_LEN_THRESHOLD
                value: "10"
              # Prefill scoring configuration
              - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
                value: "true"
              - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
                value: "true"
              - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
                value: "true"
              - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
                value: "false"
              - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
                value: "1"
              - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
                value: llm-d-operator-redis-master.llm-d.svc.cluster.local:8100
              - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
                value: "1"
              - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
                value: "1"
              - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
                value: "1"
              - name: PREFIX_AWARE_SCORER_WEIGHT
                value: "2"
              - name: SESSION_AWARE_SCORER_WEIGHT
                value: "1"
              
              {{- if .HFModelName }}
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: llm-d-hf-token
                    key: HF_TOKEN
              {{- end }}
              image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.1.0
              imagePullPolicy: Always
              resources:
                requests:
                  cpu: 256m
                  memory: 500Mi
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: "envoy.service.ext_proc.v3.ExternalProcessor"
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: "envoy.service.ext_proc.v3.ExternalProcessor"
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              name: epp
              ports:
                - name: grpc
                  containerPort: 9002
                  protocol: TCP
                - name: grpc-health
                  containerPort: 9003
                  protocol: TCP
                - name: metrics
                  containerPort: 9090
                  protocol: TCP
  eppService: |
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        app.kubernetes.io/gateway: llm-d-operator-inference-gateway
        llmd.ai/gather-metrics: "true"
    spec:
      ports:
        - port: 9002
          protocol: TCP
          name: grpc
        - port: 9003
          protocol: TCP
          name: grpc-health
        - port: 9090
          protocol: TCP
          name: metrics
      type: NodePort
      selector:
        app.kubernetes.io/gateway: llm-d-operator-inference-gateway
  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: 8000
  prefillDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          tolerations:
            - effect: NoSchedule
              key: nvidia.com/gpu
              operator: Exists
          containers:
            - name: vllm
              image: ghcr.io/llm-d/llm-d:v0.2.0
              imagePullPolicy: IfNotPresent
              securityContext:
                allowPrivilegeEscalation: false
              command:
                - vllm
                - serve
                - {{ default (print "/models/" .ModelPath) .HFModelName }}
              args:
                - "--port"
                - "8000"
                # HYBRID: Enable both local prefix caching AND minimal KV transfer for indexing
                - "--enable-prefix-caching"
                - "--prefix-caching-hash-algo"
                - "builtin"
                - "--gpu-memory-utilization"
                - "0.8"
                - "--max-model-len"
                - "4096"
                - "--block-size"
                - "16"
                - "--no-enable-chunked-prefill"
                # Enable KV cache metrics reporting via environment variables
              env:
                - name: HOME
                  value: /home
                - name: VLLM_LOGGING_LEVEL
                  value: INFO
                # Minimal NIXL config for sidecar communication only
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: UCX_TLS
                  value: "^cuda_ipc"
                - name: PYTHONHASHSEED
                  value: "42"
                {{- if .HFModelName }}
                - name: HF_HUB_CACHE
                  value: /models
                {{- end }}
              startupProbe:
                httpGet:
                  path: /health
                  port: 8000
                failureThreshold: 60
                initialDelaySeconds: 15
                periodSeconds: 30
                timeoutSeconds: 5
              livenessProbe:
                tcpSocket:
                  port: 8000
                failureThreshold: 3
                periodSeconds: 5
              readinessProbe:
                httpGet:
                  path: /health
                  port: 8000
                failureThreshold: 3
                periodSeconds: 5
              volumeMounts:
                - name: home
                  mountPath: /home
                - name: dshm
                  mountPath: /dev/shm
                {{- if .HFModelName }}
                - name: model-cache
                  mountPath: /models
                {{- else }}
                - name: model-storage
                  mountPath: /models
                  readOnly: true
                {{- end }}
              ports:
                - containerPort: 5557
                  protocol: TCP
                - containerPort: 80
                  protocol: TCP
          volumes:
            - name: home
              emptyDir: {}
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 1Gi
            {{ if .HFModelName }}
            - name: model-cache
              emptyDir: {}
            {{ end }}
  prefillService: |
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        llmd.ai/gather-metrics: "true"
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 5557
        protocol: TCP
      - name: vllm
        port: 8000
        protocol: TCP
