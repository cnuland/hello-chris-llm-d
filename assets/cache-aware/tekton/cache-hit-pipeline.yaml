apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: cache-hit-test
spec:
  params:
    - name: namespace
      description: "Namespace where LLM-D is deployed"
      default: "llm-d"
    - name: gateway-url
      description: "Gateway URL for testing"
      default: "http://llm-d-gateway-istio.llm-d.svc.cluster.local"
    - name: host
      description: "HTTP Host header to match HTTPRoute (empty for in-cluster/gateway default)"
      default: ""
    - name: warmup-count
      description: "Number of warm-up requests to send before measuring"
      default: "1"
    - name: prompt
      description: "Prompt to send repeatedly for cache-hit testing"
      default: "You are an expert technical consultant specializing in artificial intelligence and machine learning systems. Please provide a comprehensive analysis of the following scenario: A large enterprise is implementing a distributed inference system for their customer service chatbot that needs to handle 10,000 concurrent users across multiple geographic regions. The system requirements include: 1) Sub-200ms response latency for 95% of requests, 2) High availability with 99.9% uptime, 3) Cost optimization through efficient resource utilization, 4) Scalability to handle peak loads during business hours, and 5) Integration with existing authentication and monitoring systems. Consider the technical architecture, infrastructure requirements, deployment strategies, monitoring approaches, and potential challenges they might face during implementation. Focus on practical recommendations for achieving optimal performance and reliability."
    - name: requests
      description: "Number of measured requests to send"
      default: "30"
    - name: sleep-seconds
      description: "Seconds to sleep between requests"
      default: "0.8"
    - name: prometheus-url
      description: "Optional Prometheus base URL (e.g., http://prometheus-user-workload.openshift-user-workload-monitoring.svc.cluster.local:9091)"
      default: ""
  steps:

    - name: verify-system
      image: alpine/k8s:1.28.0
      script: |
        #!/bin/bash
        set -e
        
        echo "=== Verifying LLM-D System Status ===="
        
        # Find running decode pods with vLLM containers
        echo "Checking for running decode pods..."
        DECODE_PODS=($(kubectl get pods -n $(params.namespace) -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#DECODE_PODS[@]} -eq 0 ]; then
            echo "❌ No decode pods found. Please deploy LLM-D first."
            exit 1
        fi
        
        echo "Found ${#DECODE_PODS[@]} running decode pods:"
        for pod in "${DECODE_PODS[@]}"; do
            echo "  - $pod"
        done
        
        # Check if pods have the vLLM container with the correct image
        TEST_POD=${DECODE_PODS[0]}
        VLLM_IMAGE=$(kubectl get pod $TEST_POD -n $(params.namespace) -o jsonpath='{.spec.containers[?(@.name=="vllm")].image}')
        echo "✅ Test pod: $TEST_POD"
        echo "✅ vLLM image: $VLLM_IMAGE"
        
        # Verify pod is ready
        READY_CHECK=$(kubectl get pod $TEST_POD -n $(params.namespace) -o jsonpath='{.status.containerStatuses[?(@.name=="vllm")].ready}')
        if [[ "$READY_CHECK" == "true" ]]; then
            echo "✅ vLLM container is ready"
        else
            echo "⚠️  vLLM container may not be fully ready"
        fi

    - name: run-cache-hit-test
      image: alpine/k8s:1.28.0
      script: |
        #!/bin/bash
        set -e
        
        # Check available tools
        echo "Available tools:"
        which curl || echo "curl not available"
        which bc || echo "bc not available" 
        which jq || echo "jq not available"
        echo "Kubectl version: $(kubectl version --client)"
        
        echo "=== Production KV-Cache Validation Script ==="
        echo "Tests 90% cache hit rate with optimized vLLM v0.10.0 configuration"
        echo ""
        
        NAMESPACE="$(params.namespace)"
        GATEWAY_URL="$(params.gateway-url)"
        HOST_HEADER="$(params.host)"
        PROMPT="$(params.prompt)"
        REQUESTS="$(params.requests)"
        SLEEP_SECONDS="$(params.sleep-seconds)"
        WARMUP_COUNT="$(params.warmup-count)"
        SERVICE_METRICS_URL="http://ms-llm-d-modelservice-decode.${NAMESPACE}.svc.cluster.local:8000/metrics"
        PROM_URL="$(params.prometheus-url)"
        
        # Get current decode pods 
        DECODE_PODS=($(kubectl get pods -n $NAMESPACE -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#DECODE_PODS[@]} -eq 0 ]; then
            echo "❌ No decode pods found."
            exit 1
        fi
        
        TEST_POD=${DECODE_PODS[0]}
        echo "✅ Testing with fresh pod: $TEST_POD"
        echo ""
        
        echo "=== CONFIGURATION VERIFICATION ==="
        CONFIG_CHECK=$(kubectl logs $TEST_POD -n $NAMESPACE -c vllm | grep "non-default args" | head -1)
        echo "vLLM Configuration: $CONFIG_CHECK"
        
        if [[ $CONFIG_CHECK == *"block_size: 16"* ]] && [[ $CONFIG_CHECK == *"enable_prefix_caching: True"* ]] && [[ $CONFIG_CHECK == *"enable_chunked_prefill: False"* ]]; then
            echo "✅ Optimized configuration confirmed"
        else
            echo "⚠️  Configuration may not be fully optimized"
        fi
        echo ""
        
        echo "=== CACHE PERFORMANCE TEST ==="
        
        echo "-- Optional warm-up: $WARMUP_COUNT requests --"
        if [ "$WARMUP_COUNT" != "0" ]; then
          SESSION_ID_WARMUP="warmup-$(date +%s)"
          for i in $(seq 1 "$WARMUP_COUNT"); do
            echo "  Warm-up request $i (session: $SESSION_ID_WARMUP)..."
            curl -k -s -X POST "$GATEWAY_URL/v1/chat/completions" \
              ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
              -H "Content-Type: application/json" \
              -H "User-Agent: CacheWarmup/1.0" \
              -H "session-id: $SESSION_ID_WARMUP" \
              -H "x-session-id: $SESSION_ID_WARMUP" \
              -d "{\"model\": \"meta-llama/Llama-3.2-3B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"$PROMPT\"}], \"max_tokens\": 16, \"temperature\": 0.0, \"top_p\": 1.0, \"seed\": 12345}" > /dev/null || true
            sleep "$SLEEP_SECONDS"
          done
        fi
        
        # Helper to read metrics via the Service endpoint (legacy)
        read_metric() {
          local _port=$1
          local metric=$2
          curl -s "$SERVICE_METRICS_URL" | grep "$metric" | grep -o '[0-9.]*$' | head -1 || true
        }
        # Aggregate helper: sum a metric across the Service endpoint output (legacy)
        read_metric_agg_service() {
          local metric=$1
          curl -s "$SERVICE_METRICS_URL" | grep -E "^${metric}\{" | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}' || true
        }
        # Aggregate via Prometheus: returns integer floor of sum(metric)
        read_metric_agg_prom() {
          local prom_url="$PROM_URL"
          local metric="$1"
          if [ -z "$prom_url" ]; then echo ""; return 1; fi
          curl -s --fail -G "$prom_url/api/v1/query" --data-urlencode "query=sum(${metric})" \
            | jq -r '.data.result[0].value[1] // empty' 2>/dev/null | cut -d. -f1
        }
        # Aggregate by scraping each pod IP and summing
        read_metric_agg_pods() {
          local metric="$1"
          local sum=0
          local ips
          # List running decode pod IPs
          ips=$(kubectl get pods -n "$NAMESPACE" -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].status.podIP}')
          for ip in $ips; do
            val=$(curl -s "http://$ip:8000/metrics" | grep -E "^${metric}\{" | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}')
            val=${val%%.*}
            [ -n "$val" ] && sum=$((sum + ${val:-0}))
          done
          echo "$sum"
        }
        # Wrapper to read aggregate using Prometheus first, then pod IP fallback, then service as last resort
        read_metric_agg() {
          local metric="$1"
          local v
          v=$(read_metric_agg_prom "$metric") || true
          if [ -n "$v" ]; then echo "$v"; return; fi
          v=$(read_metric_agg_pods "$metric") || true
          if [ -n "$v" ]; then echo "$v"; return; fi
          read_metric_agg_service "$metric"
        }
        
        # Take baseline AFTER warm-up to exclude warm-up traffic (aggregate across all labels)
        AGG_BASELINE_Q_INT=$(read_metric_agg "vllm:prefix_cache_queries_total")
        AGG_BASELINE_H_INT=$(read_metric_agg "vllm:prefix_cache_hits_total")
        AGG_BASELINE_Q_INT=${AGG_BASELINE_Q_INT%%.*}
        AGG_BASELINE_H_INT=${AGG_BASELINE_H_INT%%.*}
        echo "Baseline after warm-up (aggregate) - Queries: $AGG_BASELINE_Q_INT, Hits: $AGG_BASELINE_H_INT"
        echo ""
        
        echo "Running cache-optimized test via gateway with session affinity..."
        
        echo "=== GATEWAY HEALTH CHECK ==="
        # robust health check with small retry loop
        MODELS_STATUS=""
        for try in 1 2 3; do
          MODELS_STATUS=$(curl -sk --connect-timeout 2 --max-time 5 -o /dev/null -w "%{http_code}" ${HOST_HEADER:+-H "Host: $HOST_HEADER"} "$GATEWAY_URL/v1/models" || true)
          [ "$MODELS_STATUS" = "200" ] && break
          sleep 1
        done
        echo "/v1/models HTTP status: $MODELS_STATUS"
        if [ "$MODELS_STATUS" != "200" ]; then
          echo "⚠️ /v1/models returned $MODELS_STATUS (expected 200). Continuing since EPP/Gateway may not proxy this path."
        fi
        
        # Quick /v1/chat/completions health check (log HTTP code)
        COMP_STATUS=""
        for try in 1 2 3; do
          COMP_STATUS=$(curl -sk --connect-timeout 2 --max-time 8 -o /dev/null -w "%{http_code}" ${HOST_HEADER:+-H "Host: $HOST_HEADER"} -H "Content-Type: application/json" \
            -X POST "$GATEWAY_URL/v1/chat/completions" -d '{
              "model": "meta-llama/Llama-3.2-3B-Instruct",
              "messages": [{"role": "user", "content": "healthcheck"}],
              "max_tokens": 4,
              "temperature": 0.0
            }' || true)
          [ "$COMP_STATUS" = "200" ] && break
          sleep 1
        done
        echo "/v1/chat/completions HTTP status: $COMP_STATUS"
        if [ "$COMP_STATUS" != "200" ]; then
          echo "❌ /v1/chat/completions did not return 200 (got $COMP_STATUS) — failing fast"
          exit 1
        fi
        
        # Test enhanced cache-aware routing with session headers
        # Use consistent session identifiers to leverage enhanced session-aware scoring
        SESSION_ID="enhanced-cache-test-$(date +%s)"
        
        echo "Testing enhanced cache-aware routing with session-aware scoring..."
        echo "Session ID: $SESSION_ID"
        
        # Accumulators for latency metrics
        SUM_TTFT_MS=0
        SUM_TOTAL_MS=0
        SUM_POST_MS=0
        FIRST3_TTFT_MS=0
        FIRST3_POST_MS=0
        LAST3_TTFT_MS=0
        LAST3_POST_MS=0
        
        for i in $(seq 1 "$REQUESTS"); do
            echo "  Enhanced cache test request $i (session: $SESSION_ID)..."
            # Capture timings: time_starttransfer ~ TTFT (approx), time_total = total latency
            READOUT=$(curl -k -s -o /tmp/llmd-cache-test-resp.json -w "%{time_starttransfer} %{time_total}" -X POST "$GATEWAY_URL/v1/chat/completions" \
                ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer enhanced-test-token-$SESSION_ID" \
                -H "session-id: $SESSION_ID" \
                -H "x-session-id: $SESSION_ID" \
                -H "User-Agent: EnhancedCacheTest/1.0" \
                -b "session_id=$SESSION_ID" \
                -d "{\"model\": \"meta-llama/Llama-3.2-3B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"$PROMPT\"}], \"max_tokens\": 16, \"temperature\": 0.0, \"top_p\": 1.0, \"seed\": 12345}")
            TTFT_S=$(echo "$READOUT" | awk '{print $1}')
            TOTAL_S=$(echo "$READOUT" | awk '{print $2}')
            # Convert to ms, compute post-first-byte (as TPOT approx)
            TTFT_MS=$(echo "$TTFT_S * 1000" | bc -l)
            TOTAL_MS=$(echo "$TOTAL_S * 1000" | bc -l)
            POST_MS=$(echo "($TOTAL_S - $TTFT_S) * 1000" | bc -l)
            
            # Sum up
            SUM_TTFT_MS=$(echo "$SUM_TTFT_MS + $TTFT_MS" | bc -l)
            SUM_TOTAL_MS=$(echo "$SUM_TOTAL_MS + $TOTAL_MS" | bc -l)
            SUM_POST_MS=$(echo "$SUM_POST_MS + $POST_MS" | bc -l)
            
            # Track first 3 and last 3 for improvement estimation
            if [ "$i" -le 3 ]; then
              FIRST3_TTFT_MS=$(echo "$FIRST3_TTFT_MS + $TTFT_MS" | bc -l)
              FIRST3_POST_MS=$(echo "$FIRST3_POST_MS + $POST_MS" | bc -l)
            fi
            if [ "$i" -gt $(echo "$REQUESTS - 3" | bc) ]; then
              LAST3_TTFT_MS=$(echo "$LAST3_TTFT_MS + $TTFT_MS" | bc -l)
              LAST3_POST_MS=$(echo "$LAST3_POST_MS + $POST_MS" | bc -l)
            fi
            
            # Show brief response preview
            echo "    Response: $(cat /tmp/llmd-cache-test-resp.json | jq -r '.choices[0].message.content // .choices[0].text // .error // "No response"' | head -1 | tr -d '\n' | cut -c1-50)..."
            echo "    Timings: TTFT~$(printf '%.1f' "$TTFT_MS") ms, Post~$(printf '%.1f' "$POST_MS") ms, Total~$(printf '%.1f' "$TOTAL_MS") ms"
            sleep "$SLEEP_SECONDS"
        done
        
        # Compute averages
        AVG_TTFT_MS=$(echo "$SUM_TTFT_MS / $REQUESTS" | bc -l)
        AVG_POST_MS=$(echo "$SUM_POST_MS / $REQUESTS" | bc -l)
        AVG_TOTAL_MS=$(echo "$SUM_TOTAL_MS / $REQUESTS" | bc -l)
        
        if [ "$REQUESTS" -ge 6 ]; then
          FIRST3_TTFT_AVG=$(echo "$FIRST3_TTFT_MS / 3" | bc -l)
          FIRST3_POST_AVG=$(echo "$FIRST3_POST_MS / 3" | bc -l)
          LAST3_TTFT_AVG=$(echo "$LAST3_TTFT_MS / 3" | bc -l)
          LAST3_POST_AVG=$(echo "$LAST3_POST_MS / 3" | bc -l)
          # Improvement as reduction from first3 to last3
          TTFT_IMPROVE_PCT=$(echo "scale=1; ( ($FIRST3_TTFT_AVG - $LAST3_TTFT_AVG) / $FIRST3_TTFT_AVG ) * 100" | bc -l)
          POST_IMPROVE_PCT=$(echo "scale=1; ( ($FIRST3_POST_AVG - $LAST3_POST_AVG) / $FIRST3_POST_AVG ) * 100" | bc -l)
        else
          FIRST3_TTFT_AVG=$AVG_TTFT_MS
          FIRST3_POST_AVG=$AVG_POST_MS
          LAST3_TTFT_AVG=$AVG_TTFT_MS
          LAST3_POST_AVG=$AVG_POST_MS
          TTFT_IMPROVE_PCT=0
          POST_IMPROVE_PCT=0
        fi
        
        # Compute aggregate deltas across all decode pods (final - baseline) using Service metrics only
        echo "=== AGGREGATE METRICS (Aggregated via Prometheus when available) ==="
        AGG_FINAL_Q_INT=$(read_metric_agg "vllm:prefix_cache_queries_total")
        AGG_FINAL_H_INT=$(read_metric_agg "vllm:prefix_cache_hits_total")
        AGG_FINAL_Q_INT=${AGG_FINAL_Q_INT%%.*}
        AGG_FINAL_H_INT=${AGG_FINAL_H_INT%%.*}
        DQ_INT=$((AGG_FINAL_Q_INT - AGG_BASELINE_Q_INT))
        DH_INT=$((AGG_FINAL_H_INT - AGG_BASELINE_H_INT))
        echo "Aggregate totals - Queries: $AGG_FINAL_Q_INT, Hits: $AGG_FINAL_H_INT"
        echo "Aggregate deltas (measured loop) - ΔQueries: $DQ_INT, ΔHits: $DH_INT"
        if [ "${DQ_INT:-0}" -ne 0 ]; then
          DELTA_HIT_RATE=$(awk -v h="$DH_INT" -v q="$DQ_INT" 'BEGIN{if (q==0) printf("0.0"); else printf("%.1f", (h*100)/q)}')
          echo "Delta Hit Rate (measured traffic): ${DELTA_HIT_RATE}%"
        fi
        
        # SESSION STICKINESS ANALYSIS (service metrics only cannot attribute per-pod)
        echo ""
        echo "=== SESSION STICKINESS ANALYSIS ==="
        echo "Pipeline sent $REQUESTS requests with the same session ID: $SESSION_ID"
        echo "Service-level metrics do not expose per-pod attribution; stickiness check is skipped in service-only mode."
        STICKINESS_SCORE="N/A"
        
        # Calculate overall hit rate from aggregate deltas (measured traffic only)
        if [ "${DQ_INT:-0}" -gt 0 ] 2>/dev/null; then
            OVERALL_HIT_RATE=$(awk -v h="$DH_INT" -v q="$DQ_INT" 'BEGIN{if (q==0) printf("0.0"); else printf("%.1f", (h*100)/q)}')
            echo ""
            echo "🎉 Overall Cache Hit Rate (measured): ${OVERALL_HIT_RATE}%"
            
            # Convert to integer for comparison
            HIT_RATE_INT=$(echo "$OVERALL_HIT_RATE" | cut -d. -f1)
            
            if [ "$HIT_RATE_INT" -ge "80" ] 2>/dev/null; then
                echo "🏆 EXCELLENT: High cache efficiency achieved!"
                EXIT_CODE=0
            elif [ "$HIT_RATE_INT" -ge "60" ] 2>/dev/null; then
                echo "✅ VERY GOOD: Good cache performance"
                EXIT_CODE=0
            elif [ "$HIT_RATE_INT" -ge "40" ] 2>/dev/null; then
                echo "✅ GOOD: Cache working, room for improvement"
                EXIT_CODE=0
            else
                echo "⚠️ Cache efficiency below optimal - check session affinity"
                EXIT_CODE=0
            fi
        else
            echo "⚠️ No queries detected in aggregate deltas - check configuration"
            EXIT_CODE=1
        fi
        
        echo ""
        echo "=== RESULTS ==="
        echo "Aggregate baseline queries: $AGG_BASELINE_Q_INT"
        echo "Aggregate baseline hits: $AGG_BASELINE_H_INT"
        echo "Aggregate final queries: $AGG_FINAL_Q_INT"
        echo "Aggregate final hits: $AGG_FINAL_H_INT"
        echo "Measured deltas - ΔQueries: $DQ_INT, ΔHits: $DH_INT"
        echo "Overall hit rate (measured): ${OVERALL_HIT_RATE:-0}%"
        
        echo ""
        echo "=== GATEWAY ROUTING TEST ==="
        echo "Testing cache-aware routing through production gateway..."
        
        for i in {1..5}; do
            echo "Gateway request $i..."
            RESPONSE=$(curl -k -s -X POST "$GATEWAY_URL/v1/chat/completions" \
                ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
                -H "Content-Type: application/json" \
                -d '{
                  "model": "meta-llama/Llama-3.2-3B-Instruct",
                  "messages": [{"role": "user", "content": "What makes Paris a unique city?"}],
                  "max_tokens": 8,
                  "temperature": 0.0
                }')
            echo "  Response: $(echo $RESPONSE | jq -r '.choices[0].message.content // .choices[0].text // .error // "No response"' | tr -d '\n' | cut -c1-30)..."
            sleep 0.5
        done
        
        echo ""
        echo "=== CACHE-AWARE ROUTING STATUS ==="
        echo "✅ vLLM v0.10.0 optimized configuration active"
        echo "✅ Session Affinity: 2-hour ClientIP stickiness"
        echo "✅ Cache Hit Rate: ${OVERALL_HIT_RATE:-0}%"
        echo "🎯 Production KV-Cache System Validation Complete!"
        
        exit $EXIT_CODE
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: cache-hit-pipeline
spec:
  params:
    - name: namespace
      description: "Namespace where LLM-D is deployed"
      default: "llm-d"
    - name: gateway-url
      description: "Gateway URL for testing"
      default: "http://llm-d-gateway-istio.llm-d.svc.cluster.local"
    - name: host
      description: "HTTP Host header to match HTTPRoute"
      default: "llm-d.demo.local"
    - name: warmup-count
      description: "Number of warm-up requests to send before measuring"
      default: "1"
    - name: prompt
      description: "Prompt to send repeatedly for cache-hit testing"
      default: "You are an expert technical consultant specializing in artificial intelligence and machine learning systems. Please provide a comprehensive analysis of the following scenario: A large enterprise is implementing a distributed inference system for their customer service chatbot that needs to handle 10,000 concurrent users across multiple geographic regions. The system requirements include: 1) Sub-200ms response latency for 95% of requests, 2) High availability with 99.9% uptime, 3) Cost optimization through efficient resource utilization, 4) Scalability to handle peak loads during business hours, and 5) Integration with existing authentication and monitoring systems. Consider the technical architecture, infrastructure requirements, deployment strategies, monitoring approaches, and potential challenges they might face during implementation. Focus on practical recommendations for achieving optimal performance and reliability."
    - name: requests
      description: "Number of measured requests to send"
      default: "30"
    - name: sleep-seconds
      description: "Seconds to sleep between requests"
      default: "0.8"
    - name: prometheus-url
      description: "Prometheus-compatible base URL (Thanos Querier in OpenShift works)"
      default: "http://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
  tasks:
    - name: cache-hit-test
      taskRef:
        name: cache-hit-test
      params:
        - name: namespace
          value: "$(params.namespace)"
        - name: gateway-url
          value: "$(params.gateway-url)"
        - name: host
          value: "$(params.host)"
        - name: warmup-count
          value: "$(params.warmup-count)"
        - name: prompt
          value: "$(params.prompt)"
        - name: requests
          value: "$(params.requests)"
        - name: sleep-seconds
          value: "$(params.sleep-seconds)"
        - name: prometheus-url
          value: "$(params.prometheus-url)"

