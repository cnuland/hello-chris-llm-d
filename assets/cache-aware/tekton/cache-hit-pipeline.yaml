apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: cache-hit-test
  namespace: llm-d
spec:
  params:
    - name: namespace
      description: "Namespace where LLM-D is deployed"
      default: "llm-d"
    - name: gateway-url
      description: "Gateway URL for testing"
      default: "http://llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local"
    - name: host
      description: "HTTP Host header to match HTTPRoute (empty for in-cluster/gateway default)"
      default: "llm-d-infra-inference-gateway.localhost"
    - name: warmup-count
      description: "Number of warm-up requests to send before measuring"
      default: "1"
    - name: prompt
      description: "Prompt to send repeatedly for cache-hit testing"
      default: "You are an expert technical consultant specializing in artificial intelligence and machine learning systems. Please provide a comprehensive analysis of the following scenario: A large enterprise is implementing a distributed inference system for their customer service chatbot that needs to handle 10,000 concurrent users across multiple geographic regions. The system requirements include: 1) Sub-200ms response latency for 95% of requests, 2) High availability with 99.9% uptime, 3) Cost optimization through efficient resource utilization, 4) Scalability to handle peak loads during business hours, and 5) Integration with existing authentication and monitoring systems. Consider the technical architecture, infrastructure requirements, deployment strategies, monitoring approaches, and potential challenges they might face during implementation. Focus on practical recommendations for achieving optimal performance and reliability."
    - name: requests
      description: "Number of measured requests to send"
      default: "30"
    - name: sleep-seconds
      description: "Seconds to sleep between requests"
      default: "0.8"
    - name: prometheus-url
      description: "Optional Prometheus base URL (e.g., http://prometheus-user-workload.openshift-user-workload-monitoring.svc.cluster.local:9091)"
      default: ""
  results:
    - name: session-id
      description: "Session ID used for the measured requests"
  steps:

    - name: verify-system
      image: alpine/k8s:1.28.0
      script: |
        #!/bin/bash
        set -e
        
        echo "=== Verifying LLM-D System Status ===="
        
        # Find running decode pods with vLLM containers
        echo "Checking for running decode pods..."
        DECODE_PODS=($(kubectl get pods -n $(params.namespace) -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#DECODE_PODS[@]} -eq 0 ]; then
            echo "âŒ No decode pods found. Please deploy LLM-D first."
            exit 1
        fi
        
        echo "Found ${#DECODE_PODS[@]} running decode pods:"
        for pod in "${DECODE_PODS[@]}"; do
            echo "  - $pod"
        done
        
        # Check if pods have the vLLM container with the correct image
        TEST_POD=${DECODE_PODS[0]}
        VLLM_IMAGE=$(kubectl get pod $TEST_POD -n $(params.namespace) -o jsonpath='{.spec.containers[?(@.name=="vllm")].image}')
        echo "âœ… Test pod: $TEST_POD"
        echo "âœ… vLLM image: $VLLM_IMAGE"
        
        # Verify pod is ready
        READY_CHECK=$(kubectl get pod $TEST_POD -n $(params.namespace) -o jsonpath='{.status.containerStatuses[?(@.name=="vllm")].ready}')
        if [[ "$READY_CHECK" == "true" ]]; then
            echo "âœ… vLLM container is ready"
        else
            echo "âš ï¸  vLLM container may not be fully ready"
        fi

    - name: run-cache-hit-test
      image: alpine/k8s:1.28.0
      script: |
        #!/bin/bash
        set -e
        
        # Check available tools
        echo "Available tools:"
        which curl || echo "curl not available"
        which bc || echo "bc not available" 
        which jq || echo "jq not available"
        echo "Kubectl version: $(kubectl version --client)"
        
        echo "=== Production KV-Cache Validation Script ==="
        echo "Tests 90% cache hit rate with optimized vLLM v0.10.0 configuration"
        echo ""
        
        # Use gateway service DNS directly from inside the cluster
        GATEWAY_URL="http://llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local"
        echo "Using gateway URL: $GATEWAY_URL"
        echo ""
        
        NAMESPACE="$(params.namespace)"
        GATEWAY_URL="$(params.gateway-url)"
        HOST_HEADER="$(params.host)"
        PROMPT="$(params.prompt)"
        REQUESTS="$(params.requests)"
        SLEEP_SECONDS="$(params.sleep-seconds)"
        WARMUP_COUNT="$(params.warmup-count)"
        SERVICE_METRICS_URL="http://ms-llm-d-modelservice-decode.${NAMESPACE}.svc.cluster.local:8000/metrics"
        PROM_URL="$(params.prometheus-url)"
        
        # Generate per-run Test ID for log filtering
        TEST_ID="multi-$(date +%s)"
        echo "Test-ID: ${TEST_ID}"
        
        # Get current decode pods 
        DECODE_PODS=($(kubectl get pods -n $NAMESPACE -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'))
        
        if [ ${#DECODE_PODS[@]} -eq 0 ]; then
            echo "âŒ No decode pods found."
            exit 1
        fi
        
        TEST_POD=${DECODE_PODS[0]}
        echo "âœ… Testing with fresh pod: $TEST_POD"
        echo ""
        
        echo "=== CONFIGURATION VERIFICATION ==="
        CONFIG_CHECK=$(kubectl logs $TEST_POD -n $NAMESPACE -c vllm | grep "non-default args" | head -1)
        echo "vLLM Configuration: $CONFIG_CHECK"
        
        if [[ $CONFIG_CHECK == *"block_size: 16"* ]] && [[ $CONFIG_CHECK == *"enable_prefix_caching: True"* ]] && [[ $CONFIG_CHECK == *"enable_chunked_prefill: False"* ]]; then
            echo "âœ… Optimized configuration confirmed"
        else
            echo "âš ï¸  Configuration may not be fully optimized"
        fi
        echo ""
        
        echo "=== CACHE PERFORMANCE TEST ==="
        
        echo "-- Optional warm-up: $WARMUP_COUNT requests --"
        if [ "$WARMUP_COUNT" != "0" ]; then
          SESSION_ID_WARMUP="warmup-$(date +%s)"
          for i in $(seq 1 "$WARMUP_COUNT"); do
            echo "  Warm-up request $i (session: $SESSION_ID_WARMUP)..."
            curl -k -s -X POST "$GATEWAY_URL/v1/chat/completions" \
              ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
              -H "Content-Type: application/json" \
              -H "User-Agent: CacheWarmup/1.0" \
              -H "x-test-id: $TEST_ID" \
              -H "session-id: $SESSION_ID_WARMUP" \
              -H "x-session-id: $SESSION_ID_WARMUP" \
              -d "{\"model\": \"meta-llama/Llama-3.2-3B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"$PROMPT\"}], \"max_tokens\": 16, \"temperature\": 0.0, \"top_p\": 1.0, \"seed\": 12345}" > /dev/null || true
            sleep "$SLEEP_SECONDS"
          done
        fi
        
        # Helper to read metrics via the Service endpoint (legacy)
        read_metric() {
          local _port=$1
          local metric=$2
          curl -s "$SERVICE_METRICS_URL" | grep "$metric" | grep -o '[0-9.]*$' | head -1 || true
        }
        # Aggregate helper: sum a metric across the Service endpoint output (legacy)
        read_metric_agg_service() {
          local metric=$1
          curl -s "$SERVICE_METRICS_URL" | grep -E "^${metric}\{" | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}' || true
        }
        # Aggregate via Prometheus: returns integer floor of sum(metric)
        read_metric_agg_prom() {
          local prom_url="$PROM_URL"
          local metric="$1"
          if [ -z "$prom_url" ]; then echo ""; return 1; fi
          curl -s --fail -G "$prom_url/api/v1/query" --data-urlencode "query=sum(${metric})" \
            | jq -r '.data.result[0].value[1] // empty' 2>/dev/null | cut -d. -f1
        }
        # Aggregate by scraping each pod IP and summing
        read_metric_agg_pods() {
          local metric="$1"
          local sum=0
          local ips
          # List running decode pod IPs
          ips=$(kubectl get pods -n "$NAMESPACE" -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].status.podIP}')
          for ip in $ips; do
            val=$(curl -s "http://$ip:8000/metrics" | grep -E "^${metric}\{" | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}')
            val=${val%%.*}
            [ -n "$val" ] && sum=$((sum + ${val:-0}))
          done
          echo "$sum"
        }
        # Wrapper to read aggregate using Prometheus first, then pod IP fallback, then service as last resort
        read_metric_agg() {
          local metric="$1"
          local v
          v=$(read_metric_agg_prom "$metric") || true
          if [ -n "$v" ]; then echo "$v"; return; fi
          v=$(read_metric_agg_pods "$metric") || true
          if [ -n "$v" ]; then echo "$v"; return; fi
          read_metric_agg_service "$metric"
        }
        
        # Take baseline AFTER warm-up to exclude warm-up traffic (aggregate across all labels)
        AGG_BASELINE_Q_INT=$(read_metric_agg "vllm:prefix_cache_queries_total")
        AGG_BASELINE_H_INT=$(read_metric_agg "vllm:prefix_cache_hits_total")
        AGG_BASELINE_Q_INT=${AGG_BASELINE_Q_INT%%.*}
        AGG_BASELINE_H_INT=${AGG_BASELINE_H_INT%%.*}
        echo "Baseline after warm-up (aggregate) - Queries: $AGG_BASELINE_Q_INT, Hits: $AGG_BASELINE_H_INT"
        
        # Capture per-pod baseline queries totals for stickiness attribution later
        BASELINE_POD_FILE="/tmp/baseline_pod_q.txt"
        : > "$BASELINE_POD_FILE"
        NAMES=($(kubectl get pods -n "$NAMESPACE" -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}'))
        IPS=($(kubectl get pods -n "$NAMESPACE" -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].status.podIP}'))
        declare -A IP2NAME
        for idx in "${!NAMES[@]}"; do IP2NAME["${IPS[$idx]}"]="${NAMES[$idx]}"; done
        for ip in "${IPS[@]}"; do
          # Sum all series for vllm:prefix_cache_queries_total on this pod
          val=$(curl -s "http://$ip:8000/metrics" | grep -E '^vllm:prefix_cache_queries_total\{' | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}')
          val=${val%%.*}
          echo "$ip $val" >> "$BASELINE_POD_FILE"
        done
        echo ""
        
        echo "Running cache-optimized test via gateway with session affinity..."
        
        echo "=== GATEWAY HEALTH CHECK ==="
        # robust health check with small retry loop
        MODELS_STATUS=""
        for try in 1 2 3; do
          MODELS_STATUS=$(curl -sk --connect-timeout 2 --max-time 5 -o /dev/null -w "%{http_code}" ${HOST_HEADER:+-H "Host: $HOST_HEADER"} "$GATEWAY_URL/v1/models" || true)
          [ "$MODELS_STATUS" = "200" ] && break
          sleep 1
        done
        echo "/v1/models HTTP status: $MODELS_STATUS"
        if [ "$MODELS_STATUS" != "200" ]; then
          echo "âš ï¸ /v1/models returned $MODELS_STATUS (expected 200). Continuing since EPP/Gateway may not proxy this path."
        fi
        
        # Quick /v1/chat/completions health check (log HTTP code)
        COMP_STATUS=""
        for try in 1 2 3; do
          COMP_STATUS=$(curl -sk --connect-timeout 2 --max-time 8 -o /dev/null -w "%{http_code}" ${HOST_HEADER:+-H "Host: $HOST_HEADER"} -H "Content-Type: application/json" \
            -X POST "$GATEWAY_URL/v1/chat/completions" -d '{
              "model": "meta-llama/Llama-3.2-3B-Instruct",
              "messages": [{"role": "user", "content": "healthcheck"}],
              "max_tokens": 4,
              "temperature": 0.0
            }' || true)
          [ "$COMP_STATUS" = "200" ] && break
          sleep 1
        done
        echo "/v1/chat/completions HTTP status: $COMP_STATUS"
        if [ "$COMP_STATUS" != "200" ]; then
          echo "âŒ /v1/chat/completions did not return 200 (got $COMP_STATUS) â€” failing fast"
          exit 1
        fi
        
        # Test enhanced cache-aware routing with session headers
        # Use consistent session identifiers to leverage enhanced session-aware scoring
        SESSION_ID="enhanced-cache-test-$(date +%s)"
        
        echo "Testing enhanced cache-aware routing with session-aware scoring..."
        echo "Session ID: $SESSION_ID"
        # Publish the session ID as a Tekton task result for downstream tasks
        echo -n "$SESSION_ID" > "$(results.session-id.path)"
        
        # Accumulators for latency metrics
        SUM_TTFT_MS=0
        SUM_TOTAL_MS=0
        SUM_POST_MS=0
        FIRST3_TTFT_MS=0
        FIRST3_POST_MS=0
        LAST3_TTFT_MS=0
        LAST3_POST_MS=0
        
        # file to collect upstream hosts from gateway responses
        UP_LIST=/tmp/upstreams.list
        : > "$UP_LIST"
        
        for i in $(seq 1 "$REQUESTS"); do
            echo "  Enhanced cache test request $i (session: $SESSION_ID)..."
            HDR_FILE="/tmp/llmd-headers-$i.txt"
            # Capture timings: time_starttransfer ~ TTFT (approx), time_total = total latency
            READOUT=$(curl -k -s -D "$HDR_FILE" -o /tmp/llmd-cache-test-resp.json -w "%{time_starttransfer} %{time_total}" -X POST "$GATEWAY_URL/v1/chat/completions" \
                ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer enhanced-test-token-$SESSION_ID" \
                -H "x-test-id: $TEST_ID" \
                -H "session-id: $SESSION_ID" \
                -H "x-session-id: $SESSION_ID" \
                -H "User-Agent: EnhancedCacheTest/1.0" \
                -b "session_id=$SESSION_ID" \
                -d "{\"model\": \"meta-llama/Llama-3.2-3B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"$PROMPT\"}], \"max_tokens\": 16, \"temperature\": 0.0, \"top_p\": 1.0, \"seed\": 12345}")
            TTFT_S=$(echo "$READOUT" | awk '{print $1}')
            TOTAL_S=$(echo "$READOUT" | awk '{print $2}')
            # Convert to ms, compute post-first-byte (as TPOT approx)
            TTFT_MS=$(echo "$TTFT_S * 1000" | bc -l)
            TOTAL_MS=$(echo "$TOTAL_S * 1000" | bc -l)
            POST_MS=$(echo "($TOTAL_S - $TTFT_S) * 1000" | bc -l)
            
            # Sum up
            SUM_TTFT_MS=$(echo "$SUM_TTFT_MS + $TTFT_MS" | bc -l)
            SUM_TOTAL_MS=$(echo "$SUM_TOTAL_MS + $TOTAL_MS" | bc -l)
            SUM_POST_MS=$(echo "$SUM_POST_MS + $POST_MS" | bc -l)
            
            # Track first 3 and last 3 for improvement estimation
            if [ "$i" -le 3 ]; then
              FIRST3_TTFT_MS=$(echo "$FIRST3_TTFT_MS + $TTFT_MS" | bc -l)
              FIRST3_POST_MS=$(echo "$FIRST3_POST_MS + $POST_MS" | bc -l)
            fi
            if [ "$i" -gt $(echo "$REQUESTS - 3" | bc) ]; then
              LAST3_TTFT_MS=$(echo "$LAST3_TTFT_MS + $TTFT_MS" | bc -l)
              LAST3_POST_MS=$(echo "$LAST3_POST_MS + $POST_MS" | bc -l)
            fi
            
            # Extract upstream host from response headers and collect
            UP=$(grep -i '^x-upstream-host:' "$HDR_FILE" | awk -F': ' '{print $2}' | tr -d '\r')
            [ -n "$UP" ] && echo "$UP" >> "$UP_LIST"
            
            # Show brief response preview
            echo "    Response: $(cat /tmp/llmd-cache-test-resp.json | jq -r '.choices[0].message.content // .choices[0].text // .error // "No response"' | head -1 | tr -d '\n' | cut -c1-50)..."
            echo "    Timings: TTFT~$(printf '%.1f' "$TTFT_MS") ms, Post~$(printf '%.1f' "$POST_MS") ms, Total~$(printf '%.1f' "$TOTAL_MS") ms"
            [ -n "$UP" ] && echo "    Upstream: $UP" || true
            # Show a subset of response headers for diagnostics
            echo "    Headers (subset):"
            grep -Ei '^(x-|server:|via:|date:|x-envoy-)' "$HDR_FILE" || true
            sleep "$SLEEP_SECONDS"
        done
        
        # Compute per-pod stickiness from metrics deltas (robust, header-independent)
        echo ""
        echo "=== PER-POD STICKINESS FROM METRICS DELTAS ==="
        FINAL_POD_FILE="/tmp/final_pod_q.txt"
        : > "$FINAL_POD_FILE"
        # Re-list current running decode pod IPs (in case of restarts)
        IPS_NOW=($(kubectl get pods -n "$NAMESPACE" -l 'llm-d.ai/role=decode' --field-selector=status.phase=Running -o jsonpath='{.items[*].status.podIP}'))
        for ip in "${IPS_NOW[@]}"; do
          val=$(curl -s "http://$ip:8000/metrics" | grep -E '^vllm:prefix_cache_queries_total\{' | grep -o '[0-9.]*$' | awk '{s+=$1} END {print s+0}')
          val=${val%%.*}
          echo "$ip $val" >> "$FINAL_POD_FILE"
        done
        
        # Build baseline map
        declare -A BASE_Q
        while read -r bip bq; do
          BASE_Q["$bip"]=${bq:-0}
        done < "$BASELINE_POD_FILE"
        
        # Compute per-pod deltas and summarize
        TOTAL_DELTA=0
        TOP_DELTA=0
        TOP_IP="unknown"
        echo "Per-pod query deltas (ip->delta, name):"
        while read -r fip fq; do
          bq=${BASE_Q["$fip"]:-0}
          d=$(( ${fq:-0} - ${bq:-0} ))
          # Only count positive deltas for the measured loop
          if [ "$d" -lt 0 ]; then d=0; fi
          TOTAL_DELTA=$((TOTAL_DELTA + d))
          name="${IP2NAME[$fip]:-unknown}"
          printf "  %s -> %d  (%s)\n" "$fip" "$d" "$name"
          if [ "$d" -gt "$TOP_DELTA" ]; then TOP_DELTA=$d; TOP_IP=$fip; fi
        done < "$FINAL_POD_FILE"
        if [ "${TOTAL_DELTA:-0}" -gt 0 ]; then
          STICKINESS_PCT_METRICS=$(awk -v t="$TOP_DELTA" -v n="$TOTAL_DELTA" 'BEGIN{if(n==0) printf("0.0"); else printf("%.1f", (t*100.0)/n)}')
        else
          STICKINESS_PCT_METRICS="0.0"
        fi
        echo "TopPodIP $TOP_IP  TopCount $TOP_DELTA  Total $TOTAL_DELTA  Stickiness % (metrics) $STICKINESS_PCT_METRICS"
        
        # Compute stickiness from captured upstream hosts (response headers)
        echo ""
        echo "=== SESSION STICKINESS (from x-upstream-host headers) ==="
        TOTAL_UP=0
        declare -A UPC
        while read -r up; do
          [ -z "$up" ] && continue
          UPC[$up]=$(( ${UPC[$up]:-0} + 1 ))
          TOTAL_UP=$((TOTAL_UP + 1))
        done < "$UP_LIST"
        TOPUP="unknown"
        TOPCNT=0
        for up in "${!UPC[@]}"; do
          c=${UPC[$up]}
          if [ "$c" -gt "$TOPCNT" ]; then TOPCNT=$c; TOPUP=$up; fi
        done
        if [ "${TOTAL_UP:-0}" -gt 0 ]; then
          STICKINESS_PCT=$(awk -v t="$TOPCNT" -v n="$TOTAL_UP" 'BEGIN{if(n==0) printf("0.0"); else printf("%.1f", (t*100.0)/n)}')
        else
          STICKINESS_PCT="0.0"
        fi
        echo "Session $SESSION_ID  TopUpstream $TOPUP  TopCount $TOPCNT  Total $TOTAL_UP  Stickiness % $STICKINESS_PCT"
        
        # Compute averages
        AVG_TTFT_MS=$(echo "$SUM_TTFT_MS / $REQUESTS" | bc -l)
        AVG_POST_MS=$(echo "$SUM_POST_MS / $REQUESTS" | bc -l)
        AVG_TOTAL_MS=$(echo "$SUM_TOTAL_MS / $REQUESTS" | bc -l)
        
        if [ "$REQUESTS" -ge 6 ]; then
          FIRST3_TTFT_AVG=$(echo "$FIRST3_TTFT_MS / 3" | bc -l)
          FIRST3_POST_AVG=$(echo "$FIRST3_POST_MS / 3" | bc -l)
          LAST3_TTFT_AVG=$(echo "$LAST3_TTFT_MS / 3" | bc -l)
          LAST3_POST_AVG=$(echo "$LAST3_POST_MS / 3" | bc -l)
          # Improvement as reduction from first3 to last3
          TTFT_IMPROVE_PCT=$(echo "scale=1; ( ($FIRST3_TTFT_AVG - $LAST3_TTFT_AVG) / $FIRST3_TTFT_AVG ) * 100" | bc -l)
          POST_IMPROVE_PCT=$(echo "scale=1; ( ($FIRST3_POST_AVG - $LAST3_POST_AVG) / $FIRST3_POST_AVG ) * 100" | bc -l)
        else
          FIRST3_TTFT_AVG=$AVG_TTFT_MS
          FIRST3_POST_AVG=$AVG_POST_MS
          LAST3_TTFT_AVG=$AVG_TTFT_MS
          LAST3_POST_AVG=$AVG_POST_MS
          TTFT_IMPROVE_PCT=0
          POST_IMPROVE_PCT=0
        fi
        
        # Compute aggregate deltas across all decode pods (final - baseline) using Service metrics only
        echo "=== AGGREGATE METRICS (Aggregated via Prometheus when available) ==="
        AGG_FINAL_Q_INT=$(read_metric_agg "vllm:prefix_cache_queries_total")
        AGG_FINAL_H_INT=$(read_metric_agg "vllm:prefix_cache_hits_total")
        AGG_FINAL_Q_INT=${AGG_FINAL_Q_INT%%.*}
        AGG_FINAL_H_INT=${AGG_FINAL_H_INT%%.*}
        DQ_INT=$((AGG_FINAL_Q_INT - AGG_BASELINE_Q_INT))
        DH_INT=$((AGG_FINAL_H_INT - AGG_BASELINE_H_INT))
        echo "Aggregate totals - Queries: $AGG_FINAL_Q_INT, Hits: $AGG_FINAL_H_INT"
        echo "Aggregate deltas (measured loop) - Î”Queries: $DQ_INT, Î”Hits: $DH_INT"
        if [ "${DQ_INT:-0}" -ne 0 ]; then
          DELTA_HIT_RATE=$(awk -v h="$DH_INT" -v q="$DQ_INT" 'BEGIN{if (q==0) printf("0.0"); else printf("%.1f", (h*100)/q)}')
          echo "Delta Hit Rate (measured traffic): ${DELTA_HIT_RATE}%"
        fi
        
        # SESSION STICKINESS ANALYSIS (service metrics only cannot attribute per-pod)
        echo ""
        echo "=== SESSION STICKINESS ANALYSIS ==="
        echo "Pipeline sent $REQUESTS requests with the same session ID: $SESSION_ID"
        echo "Service-level metrics do not expose per-pod attribution; stickiness check is skipped in service-only mode."
        STICKINESS_SCORE="N/A"
        
        # Calculate overall hit rate from aggregate deltas (measured traffic only)
        if [ "${DQ_INT:-0}" -gt 0 ] 2>/dev/null; then
            OVERALL_HIT_RATE=$(awk -v h="$DH_INT" -v q="$DQ_INT" 'BEGIN{if (q==0) printf("0.0"); else printf("%.1f", (h*100)/q)}')
            echo ""
            echo "ðŸŽ‰ Overall Cache Hit Rate (measured): ${OVERALL_HIT_RATE}%"
            
            # Convert to integer for comparison
            HIT_RATE_INT=$(echo "$OVERALL_HIT_RATE" | cut -d. -f1)
            
            if [ "$HIT_RATE_INT" -ge "80" ] 2>/dev/null; then
                echo "ðŸ† EXCELLENT: High cache efficiency achieved!"
                EXIT_CODE=0
            elif [ "$HIT_RATE_INT" -ge "60" ] 2>/dev/null; then
                echo "âœ… VERY GOOD: Good cache performance"
                EXIT_CODE=0
            elif [ "$HIT_RATE_INT" -ge "40" ] 2>/dev/null; then
                echo "âœ… GOOD: Cache working, room for improvement"
                EXIT_CODE=0
            else
                echo "âš ï¸ Cache efficiency below optimal - check session affinity"
                EXIT_CODE=0
            fi
        else
            echo "âš ï¸ No queries detected in aggregate deltas - check configuration"
            EXIT_CODE=1
        fi
        
        echo ""
        echo "=== RESULTS ==="
        echo "Aggregate baseline queries: $AGG_BASELINE_Q_INT"
        echo "Aggregate baseline hits: $AGG_BASELINE_H_INT"
        echo "Aggregate final queries: $AGG_FINAL_Q_INT"
        echo "Aggregate final hits: $AGG_FINAL_H_INT"
        echo "Measured deltas - Î”Queries: $DQ_INT, Î”Hits: $DH_INT"
        echo "Overall hit rate (measured): ${OVERALL_HIT_RATE:-0}%"
        
        echo ""
        echo "=== GATEWAY ROUTING TEST ==="
        echo "Testing cache-aware routing through production gateway..."
        
        for i in {1..5}; do
            echo "Gateway request $i..."
            RESPONSE=$(curl -k -s -X POST "$GATEWAY_URL/v1/chat/completions" \
                ${HOST_HEADER:+-H "Host: $HOST_HEADER"} \
                -H "Content-Type: application/json" \
                -H "x-test-id: $TEST_ID" \
                -d '{
                  "model": "meta-llama/Llama-3.2-3B-Instruct",
                  "messages": [{"role": "user", "content": "What makes Paris a unique city?"}],
                  "max_tokens": 8,
                  "temperature": 0.0
                }')
            echo "  Response: $(echo $RESPONSE | jq -r '.choices[0].message.content // .choices[0].text // .error // "No response"' | tr -d '\n' | cut -c1-30)..."
            sleep 0.5
        done
        
        echo ""
        echo "=== MULTI-SESSION STICKINESS SUMMARY (current Test-ID and Session-ID) ==="
        # Discover gateway pods (search all namespaces for the Helm-managed gateway pods)
        GW_JSON=$(kubectl get pods -A -l 'gateway.networking.k8s.io/gateway-name=llm-d-infra-inference-gateway' -o json)
        GW_COUNT=$(echo "$GW_JSON" | jq '.items | length')
        echo "Found $GW_COUNT gateway pod(s) for llm-d-infra-inference-gateway"
        
        # Give the gateway a moment to flush access logs from this run
        sleep 15

        # Collect last 60m logs from gateway into a temp file
        TMP_LOG=$(mktemp)
        echo "$GW_JSON" | jq -r '.items[] | [.metadata.namespace, .metadata.name] | @tsv' | while IFS=$'\t' read -r GNS GNAME; do
          kubectl logs -n "$GNS" "$GNAME" --since=60m --tail=20000 || true
        done > "$TMP_LOG"
        
        # Count requests per upstream for this session & test ID
        TOTAL=0
        declare -A COUNTS
        
        # Parse JSON logs looking for our test_id and session_id
        while read -r line; do
          # Try to parse as JSON, skip if fails
          if ! echo "$line" | jq -e . >/dev/null 2>&1; then
            continue
          fi
          
          # Extract fields
          TEST_ID_LOG=$(echo "$line" | jq -r '.x_test_id // empty')
          SESSION_ID_LOG=$(echo "$line" | jq -r '.x_session_id // empty')
          UPSTREAM=$(echo "$line" | jq -r '.upstream_host // empty')
          
          # Skip if not our test ID or session ID
          [ "$TEST_ID_LOG" != "$TEST_ID" ] && continue
          [ "$SESSION_ID_LOG" != "$SESSION_ID" ] && continue
          [ -z "$UPSTREAM" ] && continue
          
          COUNTS["$UPSTREAM"]=$(( ${COUNTS["$UPSTREAM"]:-0} + 1 ))
          TOTAL=$((TOTAL + 1))
        done < "$TMP_LOG"
        
        # Find the top upstream
        TOPUP="unknown"
        TOPCNT=0
        for UP in "${!COUNTS[@]}"; do
          c=${COUNTS[$UP]}
          if [ "$c" -gt "$TOPCNT" ]; then
            TOPCNT=$c
            TOPUP=$UP
          fi
        done
        
        # Calculate stickiness %
        if [ "${TOTAL:-0}" -gt 0 ]; then
          STICKINESS_PCT=$(awk -v t="$TOPCNT" -v n="$TOTAL" 'BEGIN{if(n==0) printf("0.0"); else printf("%.1f", (t*100.0)/n)}')
        else
          STICKINESS_PCT="0.0"
        fi
        
        # Print summary
        printf "Session %s\tTopUpstream %s\tTopCount %d\tTotal %d\tStickiness %% %s\n" \
          "$SESSION_ID" "$TOPUP" "$TOPCNT" "${TOTAL:-0}" "$STICKINESS_PCT"
        
        echo ""
        echo "=== CACHE-AWARE ROUTING STATUS ==="
        echo "âœ… vLLM v0.10.0 optimized configuration active"
        echo "âœ… Session Affinity: 2-hour ClientIP stickiness"
        echo "âœ… Cache Hit Rate: ${OVERALL_HIT_RATE:-0}%"
        echo "ðŸŽ¯ Production KV-Cache System Validation Complete!"
        
        exit $EXIT_CODE
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: cache-hit-pipeline
  namespace: llm-d
spec:
  params:
    - name: namespace
      description: "Namespace where LLM-D is deployed"
      default: "llm-d"
    - name: gateway-url
      description: "Gateway URL for testing"
      default: "http://llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local"
    - name: host
      description: "HTTP Host header to match HTTPRoute"
      default: "llm-d-infra-inference-gateway.localhost"
    - name: warmup-count
      description: "Number of warm-up requests to send before measuring"
      default: "1"
    - name: prompt
      description: "Prompt to send repeatedly for cache-hit testing"
      default: "You are an expert technical consultant specializing in artificial intelligence and machine learning systems. Please provide a comprehensive analysis of the following scenario: A large enterprise is implementing a distributed inference system for their customer service chatbot that needs to handle 10,000 concurrent users across multiple geographic regions. The system requirements include: 1) Sub-200ms response latency for 95% of requests, 2) High availability with 99.9% uptime, 3) Cost optimization through efficient resource utilization, 4) Scalability to handle peak loads during business hours, and 5) Integration with existing authentication and monitoring systems. Consider the technical architecture, infrastructure requirements, deployment strategies, monitoring approaches, and potential challenges they might face during implementation. Focus on practical recommendations for achieving optimal performance and reliability."
    - name: requests
      description: "Number of measured requests to send"
      default: "30"
    - name: sleep-seconds
      description: "Seconds to sleep between requests"
      default: "0.8"
    - name: prometheus-url
      description: "Prometheus-compatible base URL (Thanos Querier in OpenShift works)"
      default: "http://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
  tasks:
    - name: cache-hit-test
      taskRef:
        name: cache-hit-test
      params:
        - name: namespace
          value: "$(params.namespace)"
        - name: gateway-url
          value: "$(params.gateway-url)"
        - name: host
          value: "$(params.host)"
        - name: warmup-count
          value: "$(params.warmup-count)"
        - name: prompt
          value: "$(params.prompt)"
        - name: requests
          value: "$(params.requests)"
        - name: sleep-seconds
          value: "$(params.sleep-seconds)"
        - name: prometheus-url
          value: "$(params.prometheus-url)"

