# DEPRECATED: legacy 1B service (archived under assets/cache-aware/legacy/)
# Use the canonical demo instead:
#  - assets/llm-d/
#  - assets/envoyfilter-epp.yaml
#  - assets/inference-crs.yaml
#  - assets/cache-aware/tekton/cache-hit-pipeline.yaml
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"cache-aware","llmd.ai/gather-metrics":"true"},"name":"llama-3-2-1b-cache-aware-service","namespace":"llm-d"},"spec":{"ports":[{"name":"vllm-proxy","port":8000,"protocol":"TCP","targetPort":8000},{"name":"vllm","port":8001,"protocol":"TCP","targetPort":8001}],"selector":{"llm-d.ai/model":"llama-3-2-1b","llm-d.ai/role":"decode"},"sessionAffinity":"ClientIP","sessionAffinityConfig":{"clientIP":{"timeoutSeconds":7200}},"type":"ClusterIP"}}
  creationTimestamp: "2025-08-02T23:13:57Z"
  labels:
    app: cache-aware
    llmd.ai/gather-metrics: "true"
  name: llama-3-2-1b-cache-aware-service
  namespace: llm-d
  resourceVersion: "9162159"
  uid: fecd22fb-a548-4b5b-800d-a95544f5f937
spec:
  clusterIP: 172.30.180.193
  clusterIPs:
  - 172.30.180.193
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: vllm-proxy
    port: 8000
    protocol: TCP
    targetPort: 8000
  - name: vllm
    port: 8001
    protocol: TCP
    targetPort: 8001
  selector:
    llm-d.ai/model: llama-3-2-1b
    llm-d.ai/role: decode
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 7200
  type: ClusterIP
status:
  loadBalancer: {}
