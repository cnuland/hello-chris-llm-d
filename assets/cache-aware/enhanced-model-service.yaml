apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama-3-2-1b-enhanced
  namespace: llm-d
  labels:
    app: enhanced-cache-aware-routing
spec:
  modelArtifacts:
    uri: "hf://meta-llama/Llama-3.2-1B"
    size: 50Gi
    authSecretName: "llm-d-hf-token"

  routing:
    modelName: llama-3-2-1b-enhanced
    servicePort: 8000
    proxy:
      image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.7
      secure: false
      connector: nixlv2
    parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: llm-d-gateway
      namespace: llm-d
    epp:
      create: true
      # Enhanced EPP configuration with session-aware scoring
      env:
      - name: ENABLE_KVCACHE_AWARE_SCORER
        value: "true"
      - name: ENABLE_LOAD_AWARE_SCORER
        value: "true"
      - name: ENABLE_PREFIX_AWARE_SCORER
        value: "true"
      - name: ENABLE_SESSION_AWARE_SCORER
        value: "true"
      # Enhanced scoring weights for session stickiness
      - name: KVCACHE_AWARE_SCORER_WEIGHT
        value: "10"
      - name: LOAD_AWARE_SCORER_WEIGHT
        value: "1"
      - name: PREFIX_AWARE_SCORER_WEIGHT
        value: "5"
      - name: SESSION_AWARE_SCORER_WEIGHT
        value: "20"
      # Session scoring configuration
      - name: SESSION_SCORING_ALGORITHM
        value: "sticky_hash"
      - name: SESSION_HEADER_NAMES
        value: "session-id,x-session-id,authorization"
      - name: SESSION_STICKY_DURATION
        value: "3600"
      - name: SESSION_HASH_KEY_COUNT
        value: "3"
      # KV-cache indexing with faster updates
      - name: KVCACHE_INDEXER_REDIS_ADDR
        value: llm-d-operator-redis-master.llm-d.svc.cluster.local:8100
      - name: KVCACHE_INDEX_UPDATE_INTERVAL
        value: "500ms"
      - name: KVCACHE_INDEX_BATCH_SIZE
        value: "10"
      # P/D Disaggregation ENABLED with enhanced routing
      - name: PD_ENABLED
        value: "true"
      - name: PD_PROMPT_LEN_THRESHOLD
        value: "10"
      - name: PD_CACHE_AWARE_ROUTING
        value: "true"
      # Enhanced prefill scoring
      - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
        value: "true"
      - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
        value: "true"
      - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
        value: "true"
      - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
        value: "true"
      - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
        value: "10"
      - name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
        value: llm-d-operator-redis-master.llm-d.svc.cluster.local:8100
      - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
        value: "1"
      - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
        value: "5"
      - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
        value: "15"
    httpRoute:
      create: true
    inferenceModel:
      create: true
      criticality: Critical
    inferencePool:
      create: true
      name: llama-3-2-1b-enhanced-pool

  decode:
    create: true
    replicas: 3
    acceleratorTypes:
      labelKey: nvidia.com/gpu.present
      labelValues:
      - "true"
    containers:
    - name: "vllm"
      image: "ghcr.io/llm-d/llm-d:v0.2.0"
      modelCommand: vllmServe
      args:
        # Enhanced cache-aware routing optimizations
        - "--enable-prefix-caching"
        - "--prefix-caching-hash-algo=builtin"
        - "--gpu-memory-utilization=0.7"
        - "--max-model-len=4096"
        - "--block-size=16"
        - "--no-enable-chunked-prefill"
        - "--kv-cache-dtype=auto"
        - "--max-num-seqs=256"
        - "--max-num-batched-tokens=2048"
        - "--enforce-eager"
        - "--kv-transfer-config={\"kv_connector\":\"NixlConnector\", \"kv_role\":\"kv_both\"}"
        - "--port=8001"
      env:
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
        value: "5557"
      - name: VLLM_ENABLE_CACHE_INDEX_REPORTING
        value: "true"
      - name: VLLM_CACHE_INDEX_UPDATE_INTERVAL
        value: "1"
      - name: PYTHONHASHSEED
        value: "42"
      - name: UCX_TLS
        value: "^cuda_ipc"
      - name: VLLM_LOGGING_LEVEL
        value: INFO
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
      ports:
      - containerPort: 5557
        protocol: TCP
        name: nixl
      - containerPort: 8001
        protocol: TCP
        name: vllm
      - containerPort: 8000
        protocol: TCP
        name: proxy
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
      mountModelVolume: true

  prefill:
    create: true
    replicas: 2
    containers:
    - name: "vllm"
      image: "ghcr.io/llm-d/llm-d:v0.2.0"
      modelCommand: vllmServe
      args:
        # Enhanced prefill cache-aware routing optimizations
        - "--enable-prefix-caching"
        - "--prefix-caching-hash-algo=builtin"
        - "--gpu-memory-utilization=0.7"
        - "--max-model-len=4096"
        - "--block-size=16"
        - "--no-enable-chunked-prefill"
        - "--kv-cache-dtype=auto"
        - "--max-num-seqs=128"
        - "--enforce-eager"
        - "--kv-transfer-config={\"kv_connector\":\"NixlConnector\", \"kv_role\":\"kv_both\"}"
        - "--port=8000"
      env:
      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
        value: "5557"
      - name: VLLM_ENABLE_CACHE_INDEX_REPORTING
        value: "true"
      - name: VLLM_CACHE_INDEX_UPDATE_INTERVAL
        value: "2"
      - name: PYTHONHASHSEED
        value: "42"
      - name: UCX_TLS
        value: "^cuda_ipc"
      - name: VLLM_LOGGING_LEVEL
        value: INFO
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
      ports:
      - containerPort: 8000
        protocol: TCP
        name: vllm
      - containerPort: 5557
        protocol: TCP
        name: nixl
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
      mountModelVolume: true
