apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "4"
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: llama-3-2-1b
    llm-d.ai/role: decode
  name: llama-3-2-1b-decode-v2
  namespace: llm-d
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: llama-3-2-1b
      llm-d.ai/role: decode
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      creationTimestamp: null
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: llama-3-2-1b
        llm-d.ai/role: decode
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu.present
                    operator: In
                    values:
                      - "true"
      containers:
        - args:
            - serve
            - meta-llama/Llama-3.2-1B
            - --host
            - 0.0.0.0
            - --port
            - "8001"
            - --kv-transfer-config
            - '{"kv_connector":"MultiConnector","kv_role":"kv_both","kv_connector_extra_config":{"connectors":[{"kv_connector":"NixlConnector","kv_role":"kv_both"},{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}]}}'
          command:
            - vllm
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: VLLM_NIXL_SIDE_CHANNEL_PORT
              value: "5557"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: LMCACHE_DISTRIBUTED_URL
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: UCX_TLS
              value: ^cuda_ipc
            - name: LMCACHE_ENABLE_DEBUG
              value: "True"
            - name: LMCACHE_LOCAL_CPU
              value: "True"
            - name: LMCACHE_MAX_LOCAL_CPU_SIZE
              value: "5"
            - name: LMCACHE_MAX_LOCAL_DISK_SIZE
              value: "10"
            - name: LMCACHE_CHUNK_SIZE
              value: "256"
            - name: LMCACHE_LOOKUP_URL
              value: llm-d-operator-redis-master.llm-d.svc.cluster.local:8100
            - name: HF_HUB_CACHE
              value: /models
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  key: HF_TOKEN
                  name: llm-d-hf-token
          image: ghcr.io/llm-d/llm-d:0.0.8
          imagePullPolicy: IfNotPresent
          name: vllm
          ports:
            - containerPort: 8001
              name: http
              protocol: TCP
            - containerPort: 5557
              name: nixl
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8001
            timeoutSeconds: 1
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache
        - args:
            - --port=8000
            - --vllm-port=8001
            - --connector=nixlv2
          image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.7
          imagePullPolicy: IfNotPresent
          name: routing-proxy
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 8000
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: llama-3-2-1b-sa
      serviceAccountName: llama-3-2-1b-sa
      terminationGracePeriodSeconds: 30
      tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Exists
      volumes:
        - emptyDir: {}
          name: home
        - emptyDir:
            medium: Memory
            sizeLimit: 1Gi
          name: dshm
        - emptyDir: {}
          name: model-cache
