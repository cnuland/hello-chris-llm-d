# LLM-D Architecture Deployment Assets

This directory contains assets for deploying a complete **LLM-D (Large Language Model Disaggregation)** architecture. For comprehensive architecture details and demo scenarios, see the [main README](../README.md).

## ğŸ¯ What's Included

## ğŸ—ï¸ Architecture Components

### 1. **Entry Point Pool (EPP)**
- **Purpose**: Intelligent request scheduler and router
- **Features**: 
  - âœ… P/D disaggregation enabled (`PD_ENABLED=true`)
  - âœ… Load-aware and prefix-aware scoring
  - âœ… Routes requests to appropriate prefill or decode pods

### 2. **Prefill Pods**
- **Purpose**: Handle prompt processing phase
- **Features**:
  - âœ… Proper vLLM prefix caching (`--enable-prefix-caching`)
  - âœ… NIXL KV transfer for sharing cache with decode pods
  - âœ… GPU scheduling and resource management

### 3. **Decode Pods**
- **Purpose**: Handle token generation phase
- **Features**:
  - âœ… Proper vLLM prefix caching (`--enable-prefix-caching`)
  - âœ… NIXL KV transfer for receiving cache from prefill pods
  - âœ… GPU scheduling and resource management

## ğŸ”§ Key Improvements

### Fixed Prefix Caching
- **Before**: Broken LMCache configuration causing 0% hit rate
- **After**: Proper vLLM prefix caching with `sha256` hash algorithm

### P/D Disaggregation
- **Before**: `PD_ENABLED=false` (monolithic mode)
- **After**: `PD_ENABLED=true` with proper threshold configuration

### NIXL KV Transfer
- **Purpose**: Enables cache sharing between prefill and decode pods
- **Configuration**: Both pods listen on port 5557 for KV transfer

## ğŸ“ Directory Structure

```
assets/
â”œâ”€â”€ llm-d/                     # Main LLM-D architecture
â”‚   â”œâ”€â”€ modelservice.yaml      # ModelService CRD definition
â”‚   â”œâ”€â”€ configmap-preset.yaml  # Improved ConfigMap with P/D enabled
â”‚   â””â”€â”€ kustomization.yaml     # LLM-D components
â”œâ”€â”€ monitoring/                # Prometheus/Grafana monitoring
â”œâ”€â”€ load-testing/              # Load testing jobs
â””â”€â”€ kustomization.yaml         # Main deployment configuration
```

## ğŸš€ Deployment

### Prerequisites
- LLM-D operator installed
- Istio or Gateway API configured
- GPU nodes available

### Deploy Everything
```bash
kubectl apply -k assets/
```

### Deploy Only LLM-D
```bash
kubectl apply -k assets/llm-d/
```

## ğŸ“Š Monitoring

The deployment includes:
- **Grafana Dashboard**: LLM-D Performance Dashboard with KV cache hit rate
- **Prometheus Metrics**: All vLLM and EPP metrics
- **Load Testing**: Jobs to test P/D disaggregation

## ğŸ§ª Testing P/D Disaggregation

### Run Prefix Cache Test
```bash
kubectl apply -f assets/load-testing/prefix-cache-test-job.yaml
```

### Check Metrics
```bash
# Prefill pod metrics
kubectl exec -n llm-d <prefill-pod> -c vllm -- curl -s http://localhost:8000/metrics | grep prefix_cache

# Decode pod metrics  
kubectl exec -n llm-d <decode-pod> -c vllm -- curl -s http://localhost:8001/metrics | grep prefix_cache
```

### Expected Results
With P/D disaggregation working:
- **Prefill pods**: Should see `prefix_cache_queries_total > 0`
- **Decode pods**: Should see `prefix_cache_hits_total > 0` (from KV transfer)

## ğŸ” Verification

### Check EPP Configuration
```bash
kubectl describe pod -l llm-d.ai/epp -n llm-d
# Should show: PD_ENABLED: true
```

### Check NIXL Ports
```bash
kubectl exec -n llm-d <pod> -c vllm -- netstat -tuln | grep 5557
# Should show: tcp ... :5557 ... LISTEN
```

### Check Pod Roles
```bash
kubectl get pods -n llm-d -l llm-d.ai/inferenceServing=true --show-labels
# Should show pods with role=prefill and role=decode
```

## ğŸ“ˆ Performance Benefits

1. **Proper Prefix Caching**: Eliminates 0% hit rate issues
2. **P/D Disaggregation**: Optimizes resource utilization
3. **NIXL KV Transfer**: Enables cache sharing across pods
4. **Intelligent Routing**: EPP routes based on load and cache affinity

## ğŸ› Troubleshooting

### No Cache Hits
- Ensure requests with shared prefixes hit the same pod
- Check that prefix caching is enabled in metrics
- Verify NIXL KV transfer is working

### P/D Not Working
- Verify `PD_ENABLED=true` in EPP pod
- Check EPP logs for routing decisions
- Ensure prefill and decode pods are both ready

### Pod Discovery Issues
- Check EPP logs for pod reconciliation
- Verify pod labels match EPP selectors
- Ensure InferencePool is properly configured
