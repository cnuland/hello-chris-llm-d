# This patch removes the KV transfer config that interferes with prefix caching
# Apply with: kubectl patch deployment llama-3-2-1b-decode -n llm-d --patch-file=this-file.yaml

spec:
  template:
    spec:
      containers:
      - name: vllm
        args:
        - --port
        - "8001"
        - --enable-prefix-caching
        - --prefix-caching-hash-algo
        - sha256
        - --gpu-memory-utilization
        - "0.9"
        - --max-model-len
        - "4096"
        # REMOVED: --kv-transfer-config (this was causing cache interference)
        # REMOVED: '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'
        command:
        - vllm
        - serve
        - meta-llama/Llama-3.2-1B
