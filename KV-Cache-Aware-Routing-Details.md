# KV-Cache-Aware Routing Configuration Details

This document outlines the specific configurations that enable KV-cache-aware routing in your system.

## EPP (External Processing Pod) Configuration
The following settings in the EPP deployment enable KV-cache awareness:

```yaml
# Primary KV-cache awareness enabler
- name: ENABLE_KVCACHE_AWARE_SCORER
  value: true

# KV-cache scoring weight (higher = more influence)
- name: KVCACHE_AWARE_SCORER_WEIGHT
  value: 1

# Redis connection for cache indexing
- name: KVCACHE_INDEXER_REDIS_ADDR
  value: llm-d-operator-redis-master.llm-d.svc.cluster.local:6379

# Prefill/Decode disaggregation (critical for KV-cache routing)
- name: PD_ENABLED
  value: "true"

# Prefill KV-cache awareness
- name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
  value: true
- name: PREFILL_KVCACHE_INDEXER_REDIS_ADDR
  value: llm-d-operator-redis-master.llm-d.svc.cluster.local:6379
```

## vLLM Container Configuration (Decode Pods)
These configurations enable prefix caching on the vLLM side:

```yaml
args:
  - "--enable-prefix-caching"      # Enables prefix caching
  - "--prefix-caching-hash-algo"   # Hash algorithm for cache keys
  - "sha256"

env:
  # NIXL side-channel for KV-cache transfer
  - name: VLLM_NIXL_SIDE_CHANNEL_PORT
    value: "5557"
  - name: VLLM_NIXL_SIDE_CHANNEL_HOST
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
```

## Routing Proxy Sidecar
The routing proxy with NIXL connector enables KV-cache transfer:

```yaml
args:
  - "--port=8000"
  - "--vllm-port=8001"
  - "--connector=nixlv2"    # NIXL v2 connector for KV-cache awareness
```

## Key Insight
**The critical line that enables KV-cache-aware routing is:**
```yaml
- name: ENABLE_KVCACHE_AWARE_SCORER
  value: true
```
This setting in the EPP container tells the inference scheduler to consider KV-cache state when making routing decisions. Combined with the Redis indexer and NIXL side-channel communication, this creates the complete KV-cache-aware routing system that achieved your 86% cache hit rate.

## How the Operator Creates the EPP

The EPP (External Processing Pod) is generated by the operator, not deployed manually. Here's how it works:

### 1. ModelService Resource
The `ModelService` YAML file is the starting point. It specifies configurations for model artifacts, decoding, prefill, and routing. The key line here is the `baseConfigMapRef` which points to the specific ConfigMap containing deployment specifics.

### 2. ConfigMap Reference
The `baseConfigMapRef` in the `ModelService` points to the ConfigMap (e.g., `basic-gpu-with-hybrid-cache`). This ConfigMap holds the EPP configurations, including environment variables and args that enable KV-cache-aware routing.

### 3. Automatic Deployment
The operator uses the information from this ConfigMap to automatically generate and manage the EPP deployment. You don't deploy the EPP manually; instead, the operator handles the EPP lifecycle based on the configurations provided in the ModelService and its referenced ConfigMap.

### Key Parameters in ModelService:
- **`baseConfigMapRef`**: Specifies which configurations to use for deploying the EPP.
- **`routing` and `modelName`**: Ensure traffic routing and align the EPP with your model's naming.

The operator reads these configurations and translates them into a working deployment that adheres to the specified parameters.

### Complete ModelService Configuration

```yaml
apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: llama-3-2-1b
  namespace: llm-d
spec:
  # Reference to the ConfigMap with KV-cache-aware routing configurations
  baseConfigMapRef:
    name: basic-gpu-with-hybrid-cache
  
  # Model configuration
  modelArtifacts:
    uri: "hf://meta-llama/Llama-3.2-1B"
    size: 50Gi
    authSecretName: "llm-d-hf-token"
  
  # Decode phase configuration (GPU-enabled with KV-cache routing)
  decode:
    replicas: 3
    acceleratorTypes:
      labelKey: nvidia.com/gpu.present
      labelValues:
      - "true"
    containers:
    - name: vllm
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
  
  # Prefill phase configuration (disaggregated from decode)
  prefill:
    replicas: 2
    containers:
    - name: vllm
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            key: HF_TOKEN
            name: llm-d-hf-token
  
  # Routing configuration (enables EPP deployment)
  routing:
    modelName: llama-3-2-1b
    gatewayRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: llm-d-gateway
      namespace: llm-d
```

**Critical Components:**
1. **`baseConfigMapRef`**: Points to the ConfigMap containing EPP configuration with KV-cache-aware scoring
2. **`decode.replicas: 3`**: Multiple decode pods for cache distribution and session affinity
3. **`prefill.replicas: 2`**: Disaggregated prefill pods (P/D separation enables KV-cache routing)
4. **`routing.gatewayRefs`**: Connects to the Gateway that provides external access with session affinity
5. **GPU resources**: Ensures decode pods have GPU access for efficient inference

## System Workflow
The system works by:

1. EPP queries Redis for cache metadata.
2. Routes requests to pods with relevant cached prefixes.
3. NIXL side-channel transfers KV-cache data between pods when needed.
4. vLLM prefix caching optimizes memory usage within each pod.
