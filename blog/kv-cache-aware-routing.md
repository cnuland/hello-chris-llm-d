# Mastering KV-Cache-Aware Routing with llm-d

*By Christopher Nuland, Technical Marketing Manager for Red Hat AI*

## Introduction

In the era of large-scale AI inference, ensuring efficiency across distributed environments is no longer optionalâ€”it's a necessity. As workloads grow, so does the need for smarter scheduling and memory reuse strategies. Enter **llm-d**, a Kubernetes-native framework for scalable, intelligent LLM inference. One of its most powerful capabilities is **KV-cache-aware routing**, which reduces latency and improves throughput by directing requests to pods that already hold relevant context in GPU memory.

In this blog post, weâ€™ll cover:

- What KV-cache-aware routing is and why it matters  
- How llm-d implements this feature with EPPs, Gateway API Inference Extension, and intelligent routing  
- The critical Kubernetes YAML assets that make it work  
- A test case showing our latest 87.4% cache hit rate  
- Where to go to learn more and get started  

![llm-d Scale and Performance](images/scale.png)
*Scaling intelligent LLM inference with KV-cache-aware routing for enterprise workloads*

---

## What Is llm-d?

**llm-d** is an open source project built by Red Hat and the AI infrastructure community to manage large-scale LLM inference using cloud-native patterns. llm-d introduces:

- Disaggregated **prefill and decode** workloads  
- **Multi-model** and multi-tenant isolation  
- **Intelligent routing** via an External Processing Pod (EPP)  
- And crucially, **KV-cache-aware routing** for memory-efficient, low-latency inference  

---

## The Problem: Stateless Inference Fails to Reuse Cache

In traditional deployments, even if KV-caches are enabled inside the model server (like vLLM), the **gateway is unaware of cache state**. That leads to:

- Round-robin routing or explicit sticky sessions  
- Frequent **cache misses**  
- Repeated compute for common prefixes  
- Unnecessary GPU memory use  

This breaks down under high concurrency or workloads with shared prompts (like RAG, chat history, or templated inputs).

---

## The Solution: KV-Cache-Aware Routing

llm-d enables **state-aware request scheduling** by introducing the **Gateway API Inference Extension (GAIE)** with an External Processing Pod (EPP), a high-performance system that maintains intelligent routing decisions based on KV-cache awareness. The key components include:

- A **Gateway API Inference Extension (GAIE) EPP** that orchestrates intelligent scoring of pods for optimal cache utilization
- An **in-memory LRU caching system** that tracks cache state across vLLM pods without external dependencies
- A **pod discovery and labeling system** that automatically identifies and monitors decode service endpoints
- A **session-aware routing algorithm** that maintains request consistency for optimal cache reuse
- A **prefix-aware scoring system** that intelligently routes requests based on prompt similarity and cache warmth

The result is a scheduling layer that routes requests to pods most likely to have relevant cached contentâ€”dramatically reducing inference times and GPU load.

![KV-Cache-Aware Routing Architecture](images/llm-d.jpg)
*Complete KV-cache-aware routing architecture showing the flow from client requests through EPP intelligent routing to decode pods with Gateway API Inference Extension coordination*

---

## Component Versions

This guide uses the latest official LLM-D community components for optimal KV-cache-aware routing performance:

- **vLLM Inference Engine**: `ghcr.io/llm-d/llm-d:v0.2.0` (includes vLLM v0.10.0)
- **Gateway API Inference Extension (GAIE) EPP**: Official LLM-D community Helm chart with `plugins-v2.yaml` configuration
- **Infrastructure**: Official `llm-d-infra` Helm chart from LLM-D community repository
- **Cache Hit Rate**: **87.4%** (production-validated with official components)
- **Session Stickiness**: **99.91%** (near-perfect routing through EPP intelligence)

## Demo configuration at a glance

- Gateway: llm-d-infra-inference-gateway-istio.llm-d.svc.cluster.local, host llm-d.demo.local
- Stickiness: GAIE EPP-driven via Envoy ext-proc on the gateway (failure_mode_allow=true)
- Cache Management: In-memory LRU caching with no Redis dependency
- Model: meta-llama/Llama-3.2-3B-Instruct
- Decode replicas: 3 (decode-only configuration for optimal cache performance)

## Prerequisites

To follow this guide, you should have:

- OpenShift or Kubernetes with GPU-enabled nodes and NVIDIA GPU Operator
- Istio 1.27.0+ installed (required for Gateway API Inference Extension support)  
- Gateway API CRDs installed (standard + inference extension)
- LLM-D infrastructure installed via the official community Helm chart (recommended approach)
- A Hugging Face token (for downloading LLaMA or other models)
- [Project Code & Performance Test on GitHub](https://github.com/cnuland/hello-chris-llm-d)
---

## ðŸ”§ Core Configurations

### Deployment path (official community approach)

- **Official Community Helm Charts**: The `llm-d-infra` Helm chart deploys the gateway and GAIE EPP with proper `plugins-v2.yaml` configuration. This is the recommended and supported deployment method.
- **Assets-based deployment**: Direct Kubernetes manifests for decode services, EPP configuration, and Istio EnvoyFilters work with the official infrastructure.
- **Note**: The LLM-D operator is deprecated and not recommended for new deployments. Use the official community Helm charts for reliable, production-ready installations.

### (1) Decode Service and Deployment: Direct Kubernetes Assets

The current architecture uses direct Kubernetes assets instead of the ModelService CR. This provides better control and eliminates dependencies on the deprecated operator.

**Decode Service Configuration:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: ms-llm-d-modelservice-decode
  namespace: llm-d
  labels:
    app: ms-llm-d-modelservice-decode
    llm-d.ai/inferenceServing: "true"
spec:
  ports:
  - name: http
    port: 8000
    protocol: TCP
    targetPort: 8000
  selector:
    app: ms-llm-d-modelservice-decode
  type: ClusterIP
```

**Decode Deployment Configuration:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ms-llm-d-modelservice-decode
  namespace: llm-d
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ms-llm-d-modelservice-decode
  template:
    metadata:
      labels:
        app: ms-llm-d-modelservice-decode
        llm-d.ai/inferenceServing: "true"
    spec:
      containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d:v0.2.0
        args:
          # KV-cache optimizations for maximum cache hit rates
          - "vllm"
          - "serve"
          - "meta-llama/Llama-3.2-3B-Instruct"
          - "--host=0.0.0.0"
          - "--port=8000"
          - "--enable-prefix-caching"
          - "--block-size=16"
          - "--gpu-memory-utilization=0.7"
          - "--max-model-len=4096"
          - "--disable-log-requests"
          - "--kv-cache-dtype=auto"
          - "--max-num-seqs=256"
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: HF_TOKEN
              name: llm-d-hf-token
        ports:
        - containerPort: 8000
          protocol: TCP
          name: http
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi
```

### (2) HTTPRoute: Gateway Integration

The HTTPRoute connects the Istio gateway to the decode service, enabling intelligent routing through the GAIE EPP.

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: ms-llm-d-modelservice
  namespace: llm-d
  labels:
    app.kubernetes.io/instance: llm-d-infra
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: llm-d-infra-inference-gateway
  hostnames:
  - "llm-d.demo.local"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: "/v1/"
    backendRefs:
    - group: ""
      kind: Service
      name: ms-llm-d-modelservice-decode
      port: 8000
```

---

## (3) EnvoyFilter: Configures Gateway for External Processing

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: epp-ext-proc
  namespace: llm-d
spec:
  workloadSelector:
    labels:
      gateway.networking.k8s.io/gateway-name: llm-d-gateway
  configPatches:
  - applyTo: CLUSTER
    match:
      context: GATEWAY
    patch:
      operation: ADD
      value:
        name: epp-ext-proc-cluster
        type: STRICT_DNS
        connect_timeout: 2s
        lb_policy: ROUND_ROBIN
        http2_protocol_options: {}
        upstream_http_protocol_options:
          auto_sni: false
          auto_san_validation: false
        load_assignment:
          cluster_name: epp-ext-proc-cluster
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: ms-llm-d-modelservice-epp.llm-d.svc.cluster.local
                    port_value: 9002
  - applyTo: HTTP_FILTER
    match:
      context: GATEWAY
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
    patch:
      operation: INSERT_FIRST
      value:
        name: envoy.filters.http.ext_proc
        typed_config:
          '@type': type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor
          failure_mode_allow: true
          grpc_service:
            envoy_grpc:
              cluster_name: epp-ext-proc-cluster
            timeout: 30s
          message_timeout: 30s
          processing_mode:
            request_header_mode: SEND
            response_header_mode: SEND
            request_body_mode: STREAMED
            response_body_mode: STREAMED
```

---

## Official Community Helm Chart Approach

The current implementation uses the official LLM-D community Helm chart which automatically provisions:

- **Infrastructure Gateway**: Istio Gateway with proper configuration
- **GAIE EPP**: External Processing Pod with `plugins-v2.yaml` configuration
- **Service Discovery**: Automatic discovery of decode services via label selectors
- **No External Dependencies**: In-memory LRU caching eliminates Redis requirements

### InferencePool and InferenceModel (Auto-created)

The EPP automatically discovers and manages inference pools based on service labels. The decode service must have the label `llm-d.ai/inferenceServing: "true"` for automatic discovery.

**EPP Service Configuration (Deployed by Helm Chart):**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-gaie-epp
  namespace: llm-d
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-d-gaie-epp
  template:
    metadata:
      labels:
        app: llm-d-gaie-epp
    spec:
      containers:
      - name: epp
        image: # Official GAIE EPP image from community
        ports:
        - containerPort: 9002
          name: grpc
        env:
        - name: PLUGINS_CONFIG_PATH
          value: "/etc/config/plugins-v2.yaml"
        volumeMounts:
        - name: config
          mountPath: /etc/config
      volumes:
      - name: config
        configMap:
          name: llm-d-gaie-epp-config
```

Validation signals:
- EPP logs show successful pod discovery: "Found decode services with labels"
- Gateway successfully routes requests through EPP
- GET /v1/models via the gateway returns model list with 200 status

---

## ðŸ” KV-Cache Indexer: Deep Architecture Dive

The **KV-Cache Indexer** is the brain of llm-d's intelligent routing system. It maintains a global, near-real-time view of KV-Cache block locality across your entire vLLM fleet. Here's how it works:

### Core Architecture Components

| Component | Purpose | Implementation |
|:----------|:--------|:---------------|
| **`kvcache.Indexer`** | Main orchestrator handling scoring requests | Coordinates all internal modules |
| **`kvevents.Pool`** | Ingests KV-cache events from vLLM pods | Sharded ZMQ worker pool for real-time event processing |
| **`kvblock.Index`** | Core data store mapping block hashes to pods | In-memory two-level LRU cache for sub-millisecond lookups |
| **`tokenization.PrefixStore`** | Caches tokenized prompt prefixes | LRU cache avoiding expensive re-tokenization |
| **`kvblock.TokenProcessor`** | Converts tokens into KV-block keys | Chunking and hashing algorithm matching vLLM exactly |
| **`kvblock.Scorer`** | Scores pods based on cache hit sequences | Longest consecutive prefix matching strategy |

### The Read Path: Intelligent Pod Scoring

When a router needs to select the best pod for a new prompt, the **Read Path** finds the pod with the longest sequence of relevant cached KV-blocks:

1. **Token Retrieval**: Check the `PrefixStore` for the longest cached token sequence for the prompt prefix
2. **Key Generation**: Convert tokens into deterministic KV-block keys that match vLLM's internal logic
3. **Index Lookup**: Query the `kvblock.Index` to find which pods have the consecutive blocks
4. **Scoring**: Rank each pod based on consecutive matching blocks from the start of the prompt
5. **Response**: Return scored pod rankings to the router

**Key Insight**: First-time prompts may return empty results while background tokenization occurs, but common prompts achieve sub-millisecond scoring.

### The Write Path: Real-Time Cache Tracking

The **Write Path** keeps the index synchronized with actual vLLM pod cache states:

1. **Event Publication**: vLLM pods publish cache events (`BlockStored`, `BlockRemoved`) via ZMQ
2. **Message Reception**: Events parsed by topic format: `kv@pod-id@model`
3. **Sharded Processing**: Pod ID hashed (FNV-1a) to ensure ordered processing per pod
4. **Event Decoding**: Worker decodes msgpack payloads containing event batches
5. **Index Updates**: Apply cache changes to the in-memory `kvblock.Index`

### Hash Compatibility & Block Generation

Critical for accuracy, the indexer **perfectly matches vLLM's content-addressing logic**:

- **Token Chunking**: Prompts tokenized and grouped into fixed chunks (default: 16)
- **Chained Hashing**: SHA-256 hash of CBOR-encoded `[parentHash, tokenChunk, extraKeys]`
- **Hash Seed Alignment**: Must match `PYTHONHASHSEED` environment variable across all vLLM pods
- **64-bit Keys**: Uses lower 64 bits of SHA-256 for efficient storage and lookup

### Performance Optimizations

**Async Tokenization**: New prompts don't block scoring requestsâ€”tokenization happens in background worker pools with cached Hugging Face tokenizer instances.

**Two-Level LRU Caching**: The index maps block keys to pod sets using nested LRU caches for both speed and memory efficiency.

**Sharded Event Processing**: Pod events processed in parallel while maintaining per-pod ordering guarantees.

---

## ðŸ”§ Component Configuration Details

### (1) GAIE EPP (Gateway API Inference Extension External Processing Pod)

The GAIE EPP integrates intelligent pod scoring with Istio's external processing capabilities using the official LLM-D community implementation:

**EPP Configuration (Deployed via Official Helm Chart):**
```yaml
# plugins-v2.yaml configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-d-gaie-epp-config
  namespace: llm-d
data:
  plugins-v2.yaml: |
    plugins:
      - name: "cache-aware-router"
        type: "external_processor"
        config:
          discovery:
            label_selector: "llm-d.ai/inferenceServing=true"
          cache:
            type: "in-memory-lru"
            max_size: 10000
          routing:
            algorithm: "prefix-aware"
            session_affinity: true
```

**EPP Service Definition:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: ms-llm-d-modelservice-epp
  namespace: llm-d
spec:
  ports:
  - name: grpc
    port: 9002
    protocol: TCP
    targetPort: 9002
  selector:
    app: llm-d-gaie-epp
  type: ClusterIP
```

### (2) vLLM Prefix Caching Configuration

Each vLLM pod is configured for optimal prefix caching performance:

```yaml
args:
  - "--enable-prefix-caching"             # Enable KV-cache prefix reuse
  - "--block-size=16"                     # Optimal block size for cache efficiency
  - "--gpu-memory-utilization=0.7"        # Reserve memory for cache storage
  - "--max-model-len=4096"                # Match expected prompt lengths
  - "--kv-cache-dtype=auto"               # Automatic cache data type optimization
env:
  - name: CUDA_VISIBLE_DEVICES            # GPU assignment for cache isolation
    value: "0"
```

### (3) EnvoyFilter for GAIE EPP Integration

Enables the GAIE EPP to intercept and route requests based on intelligent pod scoring:

```yaml
name: envoy.filters.http.ext_proc
typed_config:
  grpc_service:
    envoy_grpc:
      cluster_name: epp-ext-proc-cluster  # Cluster pointing to GAIE EPP service
  processing_mode:
    request_header_mode: SEND     # Send request headers for routing analysis
    response_header_mode: SEND    # Send response headers for session tracking
    request_body_mode: STREAMED   # Stream request bodies for prompt analysis
  failure_mode_allow: true        # Continue routing if EPP unavailable
  message_timeout: 30s            # Allow time for intelligent scoring
```

---

## Test Case and Results

To validate that KV-cache-aware routing was functioning correctly, we designed a Tekton pipeline that simulated a typical usage pattern: multiple requests with shared prefixes, such as repeated user prompts or template-based documents.

**Monitored Signals:**

- EPP logs for intelligent routing decisions
- vLLM prometheus metrics for prefix cache hits
- Tekton metrics for latency and throughput
- Grafana dashboard for comprehensive visibility

### ðŸ” Latest Validation Results
**Production Test (Pipeline: cache-hit-run-crg5p)**

**Outstanding Performance Metrics:**
- **Total Queries: 4,776**
- **Total Cache Hits: 4,176**
- **Cache Hit Rate: 87.4%** â­ (Improved from previous 86%)
- **Session Stickiness: 99.91%** ðŸŽ¯ (Exceptional - nearly perfect)

**Traffic Distribution Analysis:**
- **Primary Pod (b26rq)**: 4,772 queries (99.92% of traffic) - 87.5% hit rate
- **Secondary Pods**: Only 4 queries total (0.08% spillover)
- **Session Affinity**: Exceeded >90% target by 9.91 percentage points

These results demonstrate a **world-class KV-cache-aware routing system** with Gateway API Inference Extension, in-memory LRU caching, and intelligent EPP routing working in perfect harmony for maximum cache utilization.

### ðŸ“Š Grafana Dashboard Monitoring

To provide comprehensive observability into the KV-cache-aware routing performance, we utilized Grafana dashboards that visualize key metrics in real-time:

![Grafana KV-Cache Performance Dashboard](images/grafana-kv-cache-results.png)
*Grafana dashboard showing cache hit rates, request distribution, and system performance metrics during our latest 87.4% cache hit rate test*

**Key Dashboard Metrics Displayed:**

- **Cache Hit Rate Timeline**: Real-time visualization of cache effectiveness across all decode pods
- **Request Distribution**: Traffic routing patterns showing session affinity in action
- **Pod-Level Performance**: Individual decode pod cache statistics and GPU utilization
- **Latency Metrics**: Response time improvements from cache hits vs. cache misses
- **System Health**: Overall cluster performance and resource utilization

The dashboard confirms our latest production results:
- **Session affinity concentrated 99.92% of requests** to the primary warm pod (exceptional stickiness)
- **Cache hit rates achieved 87.4% overall** with 87.5% on the primary pod
- **GPU memory utilization stayed optimal** at 70% without thrashing (reduced from 90% for stability)
- **Response latencies showed significant improvement** for cache-hit requests with sub-150ms times

This visual monitoring validates that the KV-cache-aware routing system is performing as designed, with measurable benefits in both efficiency and performance.

---

## Why This Matters: Real-World Impact

The **87.4% cache hit rate with 99.91% session stickiness** isn't just impressive numbersâ€”they translate into tangible business value:

### ðŸ’° **Cost Savings**
- **70% reduction in compute time** for repeated prompts means 70% fewer GPU-hours billed
- For a cluster running 10 GPUs at $2/hour, that's **$336 saved per day** on redundant computation
- Cache hits use ~90% less energy than full inference, reducing cloud costs significantly

### âš¡ **User Experience**
- **Sub-second response times** for cached prompts vs 3-5 seconds for cold inference
- **Higher throughput** means supporting 3x more concurrent users with the same hardware
- **Consistent performance** even during traffic spikes

### ðŸŽ¯ **Enterprise Use Cases Where This Shines**
- **RAG pipelines**: Document chunks get cached, making follow-up questions instant
- **Customer support**: Common queries hit cache, agents get faster responses
- **Code generation**: Template-based prompts reuse cached context
- **Multi-tenant SaaS**: Shared prompt patterns benefit all users

### ðŸ“ˆ **Scaling Impact**
- Traditional round-robin routing: Cache hit rate ~20-30%, poor session stickiness
- **llm-d KV-cache-aware routing: 87.4% cache hit rate + 99.91% session stickiness**
- **That's 3x better cache efficiency with near-perfect routing**, which compounds as you scale

The bottom line: KV-cache-aware routing isn't just technically impressiveâ€”it's **economically transformative** for production LLM workloads.

---

## âš¡ TTFT Impact: Measurable Performance Gains

One of the most immediate and noticeable benefits of KV-cache-aware routing is the dramatic improvement in **Time To First Token (TTFT)**. Here's how cache hits directly translate to faster inference:

### Baseline vs. Cache-Aware Performance

| Scenario | Without Cache Routing | With KV-Cache Routing | Improvement |
|:---------|:---------------------|:---------------------|:-----------|
| **Cold Inference** | 2,850ms TTFT | 2,850ms TTFT | Baseline |
| **Warm Cache Hit** | 2,850ms TTFT | **340ms TTFT** | **88% faster** |
| **Partial Cache Hit** | 2,850ms TTFT | 1,420ms TTFT | 50% faster |

### Real-World TTFT Measurements

**Test Configuration:**
- Model: meta-llama/Llama-3.2-3B-Instruct
- Prompt Length: 1,024 tokens (typical RAG context)
- Cache Block Size: 16 tokens
- GPU: NVIDIA A100 40GB

### Why Cache Hits Improve TTFT

**1. Eliminated Prefill Computation**
- **Cache Miss**: Must compute attention for all 1,024 input tokens
- **Cache Hit**: Reuses cached KV-blocks, only computes new tokens
- **Savings**: 64 cache blocks Ã— 16 tokens = 1,024 tokens of prefill avoided

**2. Memory Access Patterns**
- **Cache Miss**: Cold GPU memory, cache misses in attention computation
- **Cache Hit**: Warm GPU memory with pre-computed attention states
- **Result**: 5-8x faster attention computation for cached portions

**3. Reduced Model Loading**
- **Cache Miss**: May require model parameter loading if pod was idle
- **Cache Hit**: Model already loaded and warm from recent inference
- **Benefit**: Eliminates 200-500ms model loading overhead

### Production TTFT Distribution

**From our 87.4% cache hit rate performance test:**

### TTFT Performance Profile (4,776 requests)

| Request Type | Count | Percentage | LLM-Instance-1 | LLM-Instance-2 | LLM-Instance-3 | Notes |
|:-------------|:------|:-----------|:---------------|:---------------|:---------------|:------|
| **Cache Hits** | 4,176 | 87.4% | 298ms | 420ms | 580ms | 85.2% avg improvement |
| **Cache Misses** | 600 | 12.6% | 2,640ms | 3,100ms | 3,450ms | Baseline performance |

| Overall System Performance | Value |
|:---------------------------|:------|
| **Weighted Average TTFT** | 587ms |
| **Without Cache TTFT** | 2,750ms |
| **System-wide Improvement** | **78.7%** |

### Enterprise Impact of TTFT Improvements

**ðŸ“± Interactive Applications**
- **Chatbots**: Sub-400ms TTFT feels instantaneous to users
- **Code Assistants**: Fast completion suggestions improve developer flow
- **Document Q&A**: Rapid responses maintain conversation momentum

**ðŸ”„ Batch Processing**
- **Content Generation**: 78% faster processing of document batches
- **Translation Services**: Higher throughput with same GPU resources
- **Data Analysis**: Rapid insights from repetitive analytical queries

**ðŸ’¡ User Perception Studies**
- **Sub-500ms TTFT**: Users perceive as "instant" response
- **500ms-1s TTFT**: Acceptable for most interactive use cases
- **>2s TTFT**: Users begin context switching, productivity drops

### Monitoring TTFT in Production

**Key Metrics to Track:**

```yaml
# Prometheus Metrics for TTFT Monitoring
ttft_cache_hit_histogram:
  buckets: [50, 100, 200, 500, 1000, 2000, 5000]
  labels: [model, cache_status, session_id]

ttft_improvement_ratio:
  calculation: (baseline_ttft - cache_hit_ttft) / baseline_ttft
  target: > 70% improvement for cache hits

cache_effectiveness_score:
  formula: (cache_hit_rate Ã— ttft_improvement_ratio)
  target: > 60% (indicating high cache utilization with good performance gains)
```

The **87.4% cache hit rate achieving 85% TTFT improvement** represents a production system where the vast majority of users experience near-instantaneous responses, fundamentally transforming the user experience from "waiting for AI" to "AI keeping up with thought."

---

## OpenShift-specific Notes

### Gateway exposure and hostnames
- You can expose the Istio Inference Gateway via an OpenShift Route.
- Ensure your HTTPRoute hostnames include the external Route/Ingress host. For in-cluster tests, use Host: llm-d.demo.local.

### SCC and sidecar injection
- If images require non-root or additional capabilities, grant anyuid/privileged SCC to the service accounts as needed, or use compliant images.
- For non-model sample apps (frontend/backend) you may disable Istio sidecar injection to avoid NET_ADMIN/NET_RAW bootstrap issues on OpenShift. These apps are optional and not core to cache-aware routing.

### EPP readiness troubleshooting
- If EPP startup is slow, consider TCP-based readiness/liveness probes or increasing initial delay.
- Healthy logs: look for reconciliation activity and disappearance of "Pool is not initialized" messages.

---

## ðŸ“š Learn More
- [Project Code & Performance Test on GitHub](https://github.com/cnuland/hello-chris-llm-d)  
- [llm-d KV Cache Manager Architecture](https://github.com/llm-d/llm-d-kv-cache-manager/blob/main/docs/architecture.md)  
- [llm-d GitHub](https://github.com/llm-d/llm-d)  
- [llm-d Operator Quickstart](https://llm-d.ai/docs/guide/Installation/prerequisites)  
- [vLLM Documentation](https://docs.vllm.ai)

