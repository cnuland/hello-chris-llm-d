apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: llm-d-gateway-class
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
  parametersRef:
    group: gateway.envoyproxy.io
    kind: EnvoyProxy
    name: llm-d-proxy-config
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: EnvoyProxy
metadata:
  name: llm-d-proxy-config
  namespace: llm-d-demo
spec:
  telemetry:
    metrics:
      prometheus:
        disable: false
      sinks:
      - type: OpenTelemetry
        openTelemetry:
          host: jaeger-collector.llm-d-demo.svc.cluster.local
          port: 14268
    tracing:
      provider:
        type: OpenTelemetry
        openTelemetry:
          resource:
            "service.name": "llm-d-gateway"
          host: jaeger-collector.llm-d-demo.svc.cluster.local
          port: 14268
  bootstrap:
    type: Replace
    value: |
      admin:
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 19000
      static_resources:
        listeners:
        - name: inference_listener
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 8080
          filter_chains:
          - filters:
            - name: envoy.filters.network.http_connection_manager
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                stat_prefix: inference_gateway
                route_config:
                  name: inference_routes
                  virtual_hosts:
                  - name: inference_backend
                    domains: ["*"]
                    routes:
                    - match:
                        prefix: "/v1/"
                      route:
                        cluster: llm_d_scheduler
                        timeout: 300s
                http_filters:
                - name: envoy.filters.http.local_ratelimit
                  typed_config:
                    "@type": type.googleapis.com/udpa.type.v1.TypedStruct
                    type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
                    value:
                      stat_prefix: inference_rate_limiter
                      token_bucket:
                        max_tokens: 100
                        tokens_per_fill: 10
                        fill_interval: 1s
                - name: envoy.filters.http.router
        clusters:
        - name: llm_d_scheduler
          connect_timeout: 30s
          type: STRICT_DNS
          lb_policy: ROUND_ROBIN
          load_assignment:
            cluster_name: llm_d_scheduler
            endpoints:
            - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: llm-d-scheduler.llm-d-demo.svc.cluster.local
                      port_value: 8080
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: llm-d-inference-gateway
  namespace: llm-d-demo
  annotations:
    gateway.envoyproxy.io/enable-tracing: "true"
    gateway.envoyproxy.io/enable-metrics: "true"
spec:
  gatewayClassName: llm-d-gateway-class
  listeners:
  - name: inference-http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
  - name: inference-https
    port: 443
    protocol: HTTPS
    tls:
      mode: Terminate
      certificateRefs:
      - name: llm-d-tls-cert
    allowedRoutes:
      namespaces:
        from: All
  - name: metrics
    port: 9090
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: Same
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: inference-routes
  namespace: llm-d-demo
spec:
  parentRefs:
  - name: llm-d-inference-gateway
    sectionName: inference-http
  - name: llm-d-inference-gateway
    sectionName: inference-https
  hostnames:
  - "llm-d-demo.local"
  - "inference.llm-d.ai"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /v1/chat/completions
    - headers:
      - name: "Content-Type"
        value: "application/json"
    backendRefs:
    - name: llm-d-scheduler
      port: 8080
      weight: 100
    filters:
    - type: RequestHeaderModifier
      requestHeaderModifier:
        add:
        - name: "X-Inference-Route"
          value: "chat"
        - name: "X-Request-ID"
          value: "{{ random_uuid() }}"
    - type: ResponseHeaderModifier
      responseHeaderModifier:
        add:
        - name: "X-Served-By"
          value: "llm-d-gateway"
  - matches:
    - path:
        type: PathPrefix
        value: /v1/completions
    backendRefs:
    - name: llm-d-scheduler
      port: 8080
      weight: 100
    filters:
    - type: RequestHeaderModifier
      requestHeaderModifier:
        add:
        - name: "X-Inference-Route"
          value: "completion"
  - matches:
    - path:
        type: PathPrefix
        value: /metrics
    backendRefs:
    - name: llm-d-scheduler
      port: 9090
      weight: 100
  - matches:
    - path:
        type: PathPrefix
        value: /health
    backendRefs:
    - name: llm-d-scheduler
      port: 8080
      weight: 100 