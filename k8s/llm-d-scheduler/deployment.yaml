apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-scheduler
  namespace: llm-d-demo
  labels:
    app: llm-d-scheduler
    component: inference-scheduler
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-d-scheduler
  template:
    metadata:
      labels:
        app: llm-d-scheduler
        component: inference-scheduler
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: llm-d-demo-sa
      containers:
      - name: scheduler
        image: llm-d/inference-scheduler:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        - containerPort: 8000
          name: admin
          protocol: TCP
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: METRICS_ENABLED
          value: "true"
        - name: TRACING_ENABLED
          value: "true"
        - name: JAEGER_ENDPOINT
          value: "http://jaeger-collector.llm-d-demo.svc.cluster.local:14268/api/traces"
        - name: SCHEDULER_CONFIG_PATH
          value: "/etc/scheduler/config.yaml"
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: config
          mountPath: /etc/scheduler
          readOnly: true
        - name: cache-data
          mountPath: /var/cache/llm-d
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: config
        configMap:
          name: llm-d-scheduler-config
      - name: cache-data
        emptyDir:
          sizeLimit: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: llm-d-scheduler
  namespace: llm-d-demo
  labels:
    app: llm-d-scheduler
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
spec:
  selector:
    app: llm-d-scheduler
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: 9090
    protocol: TCP
  - name: admin
    port: 8000
    targetPort: 8000
    protocol: TCP
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-d-scheduler-config
  namespace: llm-d-demo
data:
  config.yaml: |
    scheduler:
      algorithm: "prefix_cache_aware"
      cache_aware_routing: true
      load_balancing_strategy: "weighted_round_robin"
      metrics_collection_interval: 5
      instance_health_check_interval: 10
      cache_hit_weight: 0.7
      load_weight: 0.3
      
    filters:
      - name: "health_filter"
        enabled: true
        config:
          max_unhealthy_percentage: 10
      - name: "capacity_filter"
        enabled: true
        config:
          max_queue_length: 50
          max_memory_usage: 0.85
      - name: "cache_affinity_filter"
        enabled: true
        config:
          cache_hit_threshold: 0.6
          prefer_cache_hits: true
    
    scorers:
      - name: "cache_score"
        weight: 0.4
        enabled: true
      - name: "load_score"
        weight: 0.3
        enabled: true
      - name: "latency_score"
        weight: 0.2
        enabled: true
      - name: "resource_score"
        weight: 0.1
        enabled: true
    
    vllm_backends:
      discovery_method: "kubernetes"
      namespace: "llm-d-demo"
      label_selector: "app=vllm-server"
      metrics_port: 8000
      health_endpoint: "/health"
      metrics_endpoint: "/metrics"
      
    cache_config:
      type: "distributed"
      ttl_seconds: 3600
      max_size_mb: 1024
      eviction_policy: "lru"
      
    observability:
      metrics:
        enabled: true
        port: 9090
        path: "/metrics"
      tracing:
        enabled: true
        service_name: "llm-d-scheduler"
        jaeger_endpoint: "http://jaeger-collector.llm-d-demo.svc.cluster.local:14268"
      logging:
        level: "INFO"
        format: "json"
        
    demo_scenarios:
      cache_aware_routing:
        enabled: true
        simulation_mode: false
      disaggregated_serving:
        enabled: true
        prefill_instances: "app=vllm-prefill"
        decode_instances: "app=vllm-decode"
      multi_tenant_qos:
        enabled: true
        priority_levels: ["interactive", "standard", "batch"]
      auto_scaling:
        enabled: true
        min_instances: 2
        max_instances: 10
        target_utilization: 0.7 